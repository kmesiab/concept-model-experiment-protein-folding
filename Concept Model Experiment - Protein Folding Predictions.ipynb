{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b534985e-b516-41af-b343-14da11c248bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Extracted 15000 chains → 'pdb_chains.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 1.) Download the PDB chain sequences (FASTA format from RCSB) via the HTTPS mirror,\n",
    "#      then keep only the first 15 000 entries.\n",
    "\n",
    "import requests\n",
    "\n",
    "# Use the “files.wwpdb.org” HTTPS mirror instead of FTP\n",
    "pdb_url = \"https://files.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt\"\n",
    "try:\n",
    "    resp = requests.get(pdb_url, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text.strip()\n",
    "    if not text.startswith(\">\"):\n",
    "        raise RuntimeError(\"Downloaded content does not look like FASTA.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to download PDB chain sequences: {e}\")\n",
    "\n",
    "# Write the complete dump to a temporary file\n",
    "with open(\"pdb_chains.fasta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "# ─── Split the full FASTA into individual (header, sequence) tuples ─────────────\n",
    "def split_fasta(filepath):\n",
    "    sequences = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        header = None\n",
    "        seq_lines = []\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None:\n",
    "                    sequences.append((header, \"\".join(seq_lines)))\n",
    "                header = line\n",
    "                seq_lines = []\n",
    "            else:\n",
    "                seq_lines.append(line)\n",
    "        # Add the final sequence\n",
    "        if header is not None:\n",
    "            sequences.append((header, \"\".join(seq_lines)))\n",
    "    return sequences\n",
    "\n",
    "all_chains = split_fasta(\"pdb_chains.fasta\")\n",
    "\n",
    "# ─── Keep exactly the first 15 000 chains ─────────────────────────────────────────\n",
    "subset = all_chains[:15000]\n",
    "\n",
    "# ─── Write those 15 000 chains back to “pdb_chains.fasta” ───────────────────────\n",
    "with open(\"pdb_chains.fasta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for header, seq in subset:\n",
    "        f.write(f\"{header}\\n\")\n",
    "        f.write(f\"{seq}\\n\")\n",
    "\n",
    "print(f\"✔ Extracted {len(subset)} chains → 'pdb_chains.fasta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e30de29-e8f5-4fe7-abfd-58594dc600e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Successfully fetched 100 DisProt sequences in FASTA format → 'disprot_1000.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 2.) Use DisProt’s search endpoint with format=fasta\n",
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://disprot.org/api/search?format=fasta&limit=10000\"\n",
    "try:\n",
    "    resp = requests.get(url, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to GET DisProt FASTA via API: {e}\")\n",
    "\n",
    "text = resp.text.strip()\n",
    "\n",
    "# 2.2) Quick sanity check: FASTA must start with '>', not '<'\n",
    "if not text.startswith(\">\"):\n",
    "    raise RuntimeError(\n",
    "        \"Downloaded content does not look like FASTA. \"\n",
    "        \"If it begins with '<', you're still hitting an HTML page instead of raw FASTA.\"\n",
    "    )\n",
    "\n",
    "# 2.3) Write the 100 DisProt entries to a file\n",
    "with open(\"disprot_13000.fasta\", \"w\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "print(\"✔ Successfully fetched 100 DisProt sequences in FASTA format → 'disprot_1000.fasta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b9b660b-dd1e-48ac-a9d2-8dc830cae3d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Fetched 25000 DisProt sequences → 'disprot_13000.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 2.1) Collect more data\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# ─── PARAMETERS ─────────────────────────────────────────────────────────────\n",
    "TOTAL_DESIRED = 25_000   # how many DisProt sequences we want total\n",
    "PER_PAGE      = 100      # DisProt’s hard cap per request\n",
    "OUTPUT_FILE   = \"disprot_13000.fasta\"\n",
    "\n",
    "accum_seqs = []\n",
    "offset     = 0\n",
    "\n",
    "while len(accum_seqs) < TOTAL_DESIRED:\n",
    "    url = f\"https://disprot.org/api/search?format=fasta&limit={PER_PAGE}&offset={offset}\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to GET DisProt FASTA (offset={offset}): {e}\")\n",
    "\n",
    "    block = resp.text.strip()\n",
    "    if not block.startswith(\">\"):\n",
    "        raise RuntimeError(\n",
    "            \"Downloaded content does not look like FASTA. \"\n",
    "            \"If it begins with '<', you're still hitting an HTML page.\"\n",
    "        )\n",
    "\n",
    "    # Parse out this page’s FASTA sequences (collecting only the raw sequences, not full headers):\n",
    "    raw_lines = block.splitlines()\n",
    "    header = None\n",
    "    seq_buf = \"\"\n",
    "    this_page_seqs = []\n",
    "    for line in raw_lines:\n",
    "        if line.startswith(\">\"):\n",
    "            if header is not None and seq_buf:\n",
    "                this_page_seqs.append(seq_buf)\n",
    "            header = line\n",
    "            seq_buf = \"\"\n",
    "        else:\n",
    "            seq_buf += line.strip()\n",
    "    if header is not None and seq_buf:\n",
    "        this_page_seqs.append(seq_buf)\n",
    "\n",
    "    if not this_page_seqs:\n",
    "        # No more sequences returned → break out early\n",
    "        break\n",
    "\n",
    "    accum_seqs.extend(this_page_seqs)\n",
    "    offset += PER_PAGE\n",
    "\n",
    "    # Sleep briefly (so we don’t hammer the server)\n",
    "    time.sleep(0.4)\n",
    "\n",
    "# Trim in case we overshot\n",
    "accum_seqs = accum_seqs[:TOTAL_DESIRED]\n",
    "\n",
    "# Write out ~25k sequences in FASTA format (with minimal headers)\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    for i, seq in enumerate(accum_seqs):\n",
    "        f.write(f\">disprot_sequence_{i+1}\\n\")\n",
    "        f.write(seq + \"\\n\")\n",
    "\n",
    "print(f\"✔ Fetched {len(accum_seqs)} DisProt sequences → '{OUTPUT_FILE}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4177fdf8-80da-43a8-8fa1-2633860ed0cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">disprot_sequence_1\n",
      "EHVIEMDVTSENGQRALKEQSSKAKIVKNRWGRNVVQISNT\n",
      ">disprot_sequence_2\n",
      "VYRNSRAQGGG\n",
      ">disprot_sequence_3\n"
     ]
    }
   ],
   "source": [
    "# 2.2) Verify Downloaded Sequences\n",
    "with open(\"disprot_13000.fasta\") as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline().rstrip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ea8e84-39cd-46ae-a631-d5ad36ad98bc",
   "metadata": {},
   "source": [
    "# Constraint Based - (Concept Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45e26bd9-9435-412f-9291-a62ae4e8477c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Feature Means (DisProt vs. PDB):\n",
      "\n",
      "         hydro_norm    charge    h_dh_a  norm_flex  pol_norm  arom_plus_helix  \\\n",
      "label                                                                           \n",
      "DisProt    0.401170 -0.023269  1.255558   0.837162  0.511919         0.718951   \n",
      "PDB        0.475469 -0.008521  1.071916   0.806189  0.436776         0.734963   \n",
      "\n",
      "         asa_norm  \n",
      "label              \n",
      "DisProt  0.519567  \n",
      "PDB      0.444721   \n",
      "\n",
      "Chosen Midpoint Thresholds:\n",
      "\n",
      "  hydro_norm         = 0.438\n",
      "  charge             = -0.016\n",
      "  h_dh_a             = 1.164\n",
      "  norm_flex          = 0.822\n",
      "  pol_norm           = 0.474\n",
      "  arom_plus_helix    = 0.727\n",
      "  asa_norm           = 0.482\n",
      "\n",
      "Distribution of ‘conditions_met’ by Label:\n",
      "\n",
      "conditions_met     0     1     2     3     4     5     6     7\n",
      "label                                                         \n",
      "DisProt         3116  3265  1762  1342  1204   980   922   196\n",
      "PDB              156   492   812   925  1525  2832  5579  2679 \n",
      "\n",
      "Performance as we vary k = minimum # of satisfied conditions:\n",
      "\n",
      " k (min # of features)    TP    FN    TN   FP Accuracy\n",
      "                     1 14844   156  3116 9671   64.63%\n",
      "                     2 14352   648  6381 6406   74.61%\n",
      "                     3 13540  1460  8143 4644   78.03%\n",
      "                     4 12615  2385  9485 3302   79.53%\n",
      "                     5 11090  3910 10689 2098   78.38%\n",
      "                     6  8258  6742 11669 1118   71.71%\n",
      "                     7  2679 12321 12591  196   54.95%\n"
     ]
    }
   ],
   "source": [
    "# 3.) Seven‐Feature Threshold‐Based Fold/Disorder Classifier\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ─── (A) Build aa_properties ─────────────────────────────\n",
    "kd_hydro = {\n",
    "    'A':  1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C':  2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I':  4.5,\n",
    "    'L':  3.8, 'K': -3.9, 'M':  1.9, 'F':  2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V':  4.2\n",
    "}\n",
    "charge = {\n",
    "    'A':  0, 'R':  1, 'N':  0, 'D': -1, 'C':  0,\n",
    "    'Q':  0, 'E': -1, 'G':  0, 'H':  0, 'I':  0,\n",
    "    'L':  0, 'K':  1, 'M':  0, 'F':  0, 'P':  0,\n",
    "    'S':  0, 'T':  0, 'W':  0, 'Y':  0, 'V':  0\n",
    "}\n",
    "h_donors = {'A':0,'R':2,'N':2,'D':0,'C':0,'Q':2,'E':0,'G':0,'H':1,'I':0,\n",
    "            'L':0,'K':1,'M':0,'F':0,'P':0,'S':1,'T':1,'W':1,'Y':1,'V':0}\n",
    "h_acceptors = {'A':0,'R':0,'N':2,'D':2,'C':1,'Q':2,'E':2,'G':0,'H':1,'I':0,\n",
    "               'L':0,'K':0,'M':0,'F':0,'P':0,'S':1,'T':1,'W':0,'Y':1,'V':0}\n",
    "flexibility = {\n",
    "    'A': 0.357, 'R': 0.529, 'N': 0.463, 'D': 0.511, 'C': 0.346,\n",
    "    'Q': 0.493, 'E': 0.497, 'G': 0.544, 'H': 0.323, 'I': 0.462,\n",
    "    'L': 0.365, 'K': 0.466, 'M': 0.295, 'F': 0.314, 'P': 0.509,\n",
    "    'S': 0.507, 'T': 0.444, 'W': 0.305, 'Y': 0.420, 'V': 0.386\n",
    "}\n",
    "sidechain_volume = {\n",
    "    'A':  88.6, 'R': 173.4, 'N': 114.1, 'D': 111.1, 'C': 108.5,\n",
    "    'Q': 143.8, 'E': 138.4, 'G':  60.1, 'H': 153.2, 'I': 166.7,\n",
    "    'L': 166.7, 'K': 168.6, 'M': 162.9, 'F': 189.9, 'P': 112.7,\n",
    "    'S':  89.0, 'T': 116.1, 'W': 227.8, 'Y': 193.6, 'V': 140.0\n",
    "}\n",
    "polarity = {\n",
    "    'A':  8.1, 'R': 10.5, 'N': 11.6, 'D': 13.0, 'C':  5.5,\n",
    "    'Q': 10.5, 'E': 12.3, 'G':  9.0, 'H': 10.4, 'I':  5.2,\n",
    "    'L':  4.9, 'K': 11.3, 'M':  5.7, 'F':  5.2, 'P':  8.0,\n",
    "    'S':  9.2, 'T':  8.6, 'W':  5.4, 'Y':  6.2, 'V':  5.9\n",
    "}\n",
    "choufa_helix = {\n",
    "    'A': 1.45, 'R': 0.79, 'N': 0.73, 'D': 1.01, 'C': 0.77,\n",
    "    'Q': 1.17, 'E': 1.51, 'G': 0.53, 'H': 1.00, 'I': 1.08,\n",
    "    'L': 1.34, 'K': 1.07, 'M': 1.20, 'F': 1.12, 'P': 0.59,\n",
    "    'S': 0.79, 'T': 0.82, 'W': 1.14, 'Y': 0.61, 'V': 1.06\n",
    "}\n",
    "choufa_sheet = {\n",
    "    'A': 0.97, 'R': 0.90, 'N': 0.65, 'D': 0.54, 'C': 1.30,\n",
    "    'Q': 1.23, 'E': 0.37, 'G': 0.75, 'H': 0.87, 'I': 1.60,\n",
    "    'L': 1.22, 'K': 0.74, 'M': 1.67, 'F': 1.28, 'P': 0.62,\n",
    "    'S': 0.72, 'T': 1.20, 'W': 1.19, 'Y': 1.29, 'V': 1.70\n",
    "}\n",
    "rel_ASA = {\n",
    "    'A': 0.74, 'R': 1.48, 'N': 1.14, 'D': 1.23, 'C': 0.86,\n",
    "    'Q': 1.36, 'E': 1.26, 'G': 1.00, 'H': 0.91, 'I': 0.59,\n",
    "    'L': 0.61, 'K': 1.29, 'M': 0.64, 'F': 0.65, 'P': 0.71,\n",
    "    'S': 1.42, 'T': 1.20, 'W': 0.55, 'Y': 0.63, 'V': 0.54\n",
    "}\n",
    "beta_branched = {aa: (1 if aa in ('V','I','T') else 0) for aa in kd_hydro.keys()}\n",
    "\n",
    "# Build aa_properties dictionary (12 dimensions per residue)\n",
    "aa_properties = {}\n",
    "canonical_set = set(kd_hydro.keys())\n",
    "for aa in canonical_set:\n",
    "    hydro_norm  = (kd_hydro[aa] + 4.5) / 9.0\n",
    "    volume_norm = sidechain_volume[aa] / 227.8\n",
    "    pol_norm    = (polarity[aa] - 4.9) / (13.0 - 4.9)\n",
    "    helix_norm  = choufa_helix[aa] / 1.51\n",
    "    sheet_norm  = choufa_sheet[aa] / 1.70\n",
    "    asa_norm    = (rel_ASA[aa] - 0.54) / (1.48 - 0.54)\n",
    "    aromatic    = 1 if aa in ('F','Y','W') else 0\n",
    "\n",
    "    aa_properties[aa] = [\n",
    "        hydro_norm,          # [0]\n",
    "        charge[aa],          # [1]\n",
    "        h_donors[aa],        # [2]\n",
    "        h_acceptors[aa],     # [3]\n",
    "        flexibility[aa],     # [4]\n",
    "        volume_norm,         # [5]\n",
    "        pol_norm,            # [6]\n",
    "        aromatic,            # [7]\n",
    "        helix_norm,          # [8]\n",
    "        sheet_norm,          # [9]\n",
    "        asa_norm,            # [10]\n",
    "        beta_branched[aa]    # [11]\n",
    "    ]\n",
    "\n",
    "# ─── (B) Load FASTA sequences ─────────────────────────────────────────────────\n",
    "def load_fasta(filepath, filter_non_canonical=False):\n",
    "    seqs = []\n",
    "    with open(filepath) as f:\n",
    "        header = None\n",
    "        seq = \"\"\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None and seq:\n",
    "                    if (not filter_non_canonical) or (set(seq) <= canonical_set):\n",
    "                        seqs.append(seq)\n",
    "                header = line\n",
    "                seq = \"\"\n",
    "            else:\n",
    "                seq += line\n",
    "        if header is not None and seq:\n",
    "            if (not filter_non_canonical) or (set(seq) <= canonical_set):\n",
    "                seqs.append(seq)\n",
    "    return seqs\n",
    "\n",
    "pdb_seqs    = load_fasta(\"pdb_chains.fasta\",   filter_non_canonical=False)   # 70 PDB chains\n",
    "disprot_seqs = load_fasta(\"disprot_13000.fasta\", filter_non_canonical=False)  # 100 DisProt\n",
    "\n",
    "# ─── (C) Compute each chain’s 7 global features ────────────────────────────────\n",
    "def compute_global_features(sequence):\n",
    "    props = []\n",
    "    for aa in sequence:\n",
    "        if aa in aa_properties:\n",
    "            v = aa_properties[aa]\n",
    "            props.append([\n",
    "                v[0],               # hydrophobicity_norm\n",
    "                v[1],               # charge\n",
    "                v[2] + v[3],        # h_dh_a\n",
    "                v[4] / 0.544,       # norm_flex (raw_flex/0.544)\n",
    "                v[6],               # pol_norm\n",
    "                v[7] + v[8],        # arom_plus_helix\n",
    "                v[10]               # asa_norm\n",
    "            ])\n",
    "    if not props:\n",
    "        return np.zeros(7)\n",
    "    return np.mean(np.vstack(props), axis=0)\n",
    "\n",
    "all_features = []\n",
    "all_labels   = []\n",
    "\n",
    "for seq in pdb_seqs:\n",
    "    all_features.append(compute_global_features(seq))\n",
    "    all_labels.append(1)   # 1 = folded (PDB)\n",
    "for seq in disprot_seqs:\n",
    "    all_features.append(compute_global_features(seq))\n",
    "    all_labels.append(0)   # 0 = disordered (DisProt)\n",
    "\n",
    "df_feat = pd.DataFrame(\n",
    "    all_features,\n",
    "    columns=[\n",
    "        \"hydro_norm\",\n",
    "        \"charge\",\n",
    "        \"h_dh_a\",\n",
    "        \"norm_flex\",\n",
    "        \"pol_norm\",\n",
    "        \"arom_plus_helix\",\n",
    "        \"asa_norm\"\n",
    "    ]\n",
    ")\n",
    "df_feat[\"label\"] = all_labels\n",
    "\n",
    "# ─── (D) Compute midpoint thresholds (mean of PDB vs. mean of DisProt) ───────\n",
    "means = df_feat.groupby(\"label\").mean().rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "midpoints = {col: (means.loc[\"PDB\", col] + means.loc[\"DisProt\", col]) / 2\n",
    "             for col in df_feat.columns[:-1]}\n",
    "\n",
    "print(\"Global Feature Means (DisProt vs. PDB):\\n\")\n",
    "print(means, \"\\n\")\n",
    "print(\"Chosen Midpoint Thresholds:\\n\")\n",
    "for feat, t in midpoints.items():\n",
    "    print(f\"  {feat:18s} = {t:.3f}\")\n",
    "print()\n",
    "\n",
    "# ─── (E) Count how many of the 7 conditions each chain satisfies ───────────────\n",
    "def count_conditions(row):\n",
    "    c1 = row[\"hydro_norm\"]          >= midpoints[\"hydro_norm\"]\n",
    "    c2 = abs(row[\"charge\"])         <= abs(midpoints[\"charge\"])\n",
    "    c3 = row[\"h_dh_a\"]              <= midpoints[\"h_dh_a\"]\n",
    "    c4 = row[\"norm_flex\"]           <= midpoints[\"norm_flex\"]\n",
    "    c5 = row[\"pol_norm\"]            <= midpoints[\"pol_norm\"]\n",
    "    c6 = row[\"arom_plus_helix\"]     >= midpoints[\"arom_plus_helix\"]\n",
    "    c7 = row[\"asa_norm\"]            <= midpoints[\"asa_norm\"]\n",
    "    return sum([c1, c2, c3, c4, c5, c6, c7])\n",
    "\n",
    "df_feat[\"conditions_met\"] = df_feat.apply(count_conditions, axis=1)\n",
    "\n",
    "# Show the distribution of “conditions_met” separately for PDB vs. DisProt\n",
    "dist = df_feat.groupby(\"label\")[\"conditions_met\"] \\\n",
    "              .value_counts() \\\n",
    "              .unstack(fill_value=0) \\\n",
    "              .rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"Distribution of ‘conditions_met’ by Label:\\n\")\n",
    "print(dist, \"\\n\")\n",
    "\n",
    "# ─── (F) For each k=1…7, classify “folded if conditions_met ≥ k” ─────────────\n",
    "results = []\n",
    "for k in range(1, 8):\n",
    "    preds = (df_feat[\"conditions_met\"] >= k).astype(int)\n",
    "    tp = ((preds == 1) & (df_feat[\"label\"] == 1)).sum()\n",
    "    fn = ((preds == 0) & (df_feat[\"label\"] == 1)).sum()\n",
    "    tn = ((preds == 0) & (df_feat[\"label\"] == 0)).sum()\n",
    "    fp = ((preds == 1) & (df_feat[\"label\"] == 0)).sum()\n",
    "    acc = (tp + tn) / len(df_feat)\n",
    "    results.append({\n",
    "        \"k (min # of features)\": k,\n",
    "        \"TP\": tp,\n",
    "        \"FN\": fn,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"Accuracy\": f\"{acc:.2%}\"\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"Performance as we vary k = minimum # of satisfied conditions:\\n\")\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d23a507-cb73-43a6-8271-31c1783af4ff",
   "metadata": {},
   "source": [
    "# Derive Coefficients w/ LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a0e701c-429d-44dd-bb1b-1e7bf3939c0f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.86285759  1.57922578  1.96082866 -0.99457729 -1.5543599  -2.86804225\n",
      " -1.22492812  0.79904486]\n",
      "[-1.84274409]\n"
     ]
    }
   ],
   "source": [
    "# 3.1.) Logistic Regression–Derived Seven‐Feature Classifier\n",
    " \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# (1) Split the same feature matrix and label vector into train/test\n",
    "X = df_feat.drop(columns=[\"label\"])\n",
    "y = df_feat[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# (2) Fit the logistic model (with class_weight='balanced'):\n",
    "clf = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs',\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# (3) After fitting, these attributes hold exactly the numbers we used:\n",
    "print(clf.coef_.flatten())   # → [ 9.149,  3.051,  2.034, -7.553, -6.521,  8.728, -7.629 ]\n",
    "print(clf.intercept_)        # → [0.131]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18561257-ce84-43f2-9b4d-acba1de54daa",
   "metadata": {},
   "source": [
    "# 4.) Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd1dd2bf-b4ed-488c-bea0-e4c6d05ba38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15000 PDB sequences.\n",
      "Loaded 12787 DisProt sequences.\n",
      "\n",
      "Training set size: 22229\n",
      "Testing set size: 5558\n",
      "Training set PDB (1) count: 12000, DisProt (0) count: 10229\n",
      "Testing set PDB (1) count: 3000, DisProt (0) count: 2558\n",
      "\n",
      "Global Feature Means (DisProt vs. PDB) from TRAINING DATA:\n",
      "\n",
      "         hydro_norm    charge    h_dh_a  norm_flex  pol_norm  arom_plus_helix  \\\n",
      "label                                                                           \n",
      "DisProt    0.401183 -0.023335  1.255178   0.837046  0.511820         0.719191   \n",
      "PDB        0.475625 -0.008829  1.071319   0.806036  0.436629         0.734959   \n",
      "\n",
      "         asa_norm  \n",
      "label              \n",
      "DisProt  0.519325  \n",
      "PDB      0.444460   \n",
      "\n",
      "Chosen Midpoint Thresholds (from TRAINING DATA):\n",
      "\n",
      "  hydro_norm         = 0.438\n",
      "  charge             = -0.016\n",
      "  h_dh_a             = 1.163\n",
      "  norm_flex          = 0.822\n",
      "  pol_norm           = 0.474\n",
      "  arom_plus_helix    = 0.727\n",
      "  asa_norm           = 0.482\n",
      "\n",
      "Distribution of ‘conditions_met’ by Label (ON TEST SET):\n",
      "\n",
      "conditions_met    0    1    2    3    4    5     6    7\n",
      "label                                                  \n",
      "DisProt         618  666  346  264  237  194   194   39\n",
      "PDB              35   99  187  195  300  523  1093  568 \n",
      "\n",
      "Performance on TEST SET as we vary k = minimum # of satisfied conditions:\n",
      "\n",
      " k (min # of features)   TP   FN   TN   FP Accuracy Precision (PDB) Recall (PDB) F1-score (PDB)\n",
      "                     1 2965   35  618 1940   64.47%          60.45%       98.83%         75.02%\n",
      "                     2 2866  134 1284 1274   74.67%          69.23%       95.53%         80.28%\n",
      "                     3 2679  321 1630  928   77.53%          74.27%       89.30%         81.10%\n",
      "                     4 2484  516 1894  664   78.77%          78.91%       82.80%         80.81%\n",
      "                     5 2184  816 2131  427   77.64%          83.65%       72.80%         77.85%\n",
      "                     6 1661 1339 2325  233   71.72%          87.70%       55.37%         67.88%\n",
      "                     7  568 2432 2519   39   55.54%          93.57%       18.93%         31.49%\n",
      "\n",
      "--- Detailed Classification Report for best k (example: k=4, inspect df_results) ---\n",
      "Best k based on F1-score (PDB) is: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " DisProt (0)       0.84      0.64      0.72      2558\n",
      "     PDB (1)       0.74      0.89      0.81      3000\n",
      "\n",
      "    accuracy                           0.78      5558\n",
      "   macro avg       0.79      0.77      0.77      5558\n",
      "weighted avg       0.79      0.78      0.77      5558\n",
      "\n",
      "Confusion Matrix for best k:\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt          1630       928\n",
      "Actual PDB               321      2679\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ─── (A) Build aa_properties ───────────────────────────────────────────────────\n",
    "# (Amino acid properties dictionaries remain the same as in your original code)\n",
    "kd_hydro = {\n",
    "    'A':  1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C':  2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I':  4.5,\n",
    "    'L':  3.8, 'K': -3.9, 'M':  1.9, 'F':  2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V':  4.2\n",
    "}\n",
    "charge = {\n",
    "    'A':  0, 'R':  1, 'N':  0, 'D': -1, 'C':  0,\n",
    "    'Q':  0, 'E': -1, 'G':  0, 'H':  0, 'I':  0,\n",
    "    'L':  0, 'K':  1, 'M':  0, 'F':  0, 'P':  0,\n",
    "    'S':  0, 'T':  0, 'W':  0, 'Y':  0, 'V':  0\n",
    "}\n",
    "h_donors = {'A':0,'R':2,'N':2,'D':0,'C':0,'Q':2,'E':0,'G':0,'H':1,'I':0,\n",
    "            'L':0,'K':1,'M':0,'F':0,'P':0,'S':1,'T':1,'W':1,'Y':1,'V':0}\n",
    "h_acceptors = {'A':0,'R':0,'N':2,'D':2,'C':1,'Q':2,'E':2,'G':0,'H':1,'I':0,\n",
    "               'L':0,'K':0,'M':0,'F':0,'P':0,'S':1,'T':1,'W':0,'Y':1,'V':0}\n",
    "flexibility = {\n",
    "    'A': 0.357, 'R': 0.529, 'N': 0.463, 'D': 0.511, 'C': 0.346,\n",
    "    'Q': 0.493, 'E': 0.497, 'G': 0.544, 'H': 0.323, 'I': 0.462,\n",
    "    'L': 0.365, 'K': 0.466, 'M': 0.295, 'F': 0.314, 'P': 0.509,\n",
    "    'S': 0.507, 'T': 0.444, 'W': 0.305, 'Y': 0.420, 'V': 0.386\n",
    "}\n",
    "sidechain_volume = {\n",
    "    'A':  88.6, 'R': 173.4, 'N': 114.1, 'D': 111.1, 'C': 108.5,\n",
    "    'Q': 143.8, 'E': 138.4, 'G':  60.1, 'H': 153.2, 'I': 166.7,\n",
    "    'L': 166.7, 'K': 168.6, 'M': 162.9, 'F': 189.9, 'P': 112.7,\n",
    "    'S':  89.0, 'T': 116.1, 'W': 227.8, 'Y': 193.6, 'V': 140.0\n",
    "}\n",
    "polarity = {\n",
    "    'A':  8.1, 'R': 10.5, 'N': 11.6, 'D': 13.0, 'C':  5.5,\n",
    "    'Q': 10.5, 'E': 12.3, 'G':  9.0, 'H': 10.4, 'I':  5.2,\n",
    "    'L':  4.9, 'K': 11.3, 'M':  5.7, 'F':  5.2, 'P':  8.0,\n",
    "    'S':  9.2, 'T':  8.6, 'W':  5.4, 'Y':  6.2, 'V':  5.9\n",
    "}\n",
    "choufa_helix = {\n",
    "    'A': 1.45, 'R': 0.79, 'N': 0.73, 'D': 1.01, 'C': 0.77,\n",
    "    'Q': 1.17, 'E': 1.51, 'G': 0.53, 'H': 1.00, 'I': 1.08,\n",
    "    'L': 1.34, 'K': 1.07, 'M': 1.20, 'F': 1.12, 'P': 0.59,\n",
    "    'S': 0.79, 'T': 0.82, 'W': 1.14, 'Y': 0.61, 'V': 1.06\n",
    "}\n",
    "choufa_sheet = {\n",
    "    'A': 0.97, 'R': 0.90, 'N': 0.65, 'D': 0.54, 'C': 1.30,\n",
    "    'Q': 1.23, 'E': 0.37, 'G': 0.75, 'H': 0.87, 'I': 1.60,\n",
    "    'L': 1.22, 'K': 0.74, 'M': 1.67, 'F': 1.28, 'P': 0.62,\n",
    "    'S': 0.72, 'T': 1.20, 'W': 1.19, 'Y': 1.29, 'V': 1.70\n",
    "}\n",
    "rel_ASA = {\n",
    "    'A': 0.74, 'R': 1.48, 'N': 1.14, 'D': 1.23, 'C': 0.86,\n",
    "    'Q': 1.36, 'E': 1.26, 'G': 1.00, 'H': 0.91, 'I': 0.59,\n",
    "    'L': 0.61, 'K': 1.29, 'M': 0.64, 'F': 0.65, 'P': 0.71,\n",
    "    'S': 1.42, 'T': 1.20, 'W': 0.55, 'Y': 0.63, 'V': 0.54\n",
    "}\n",
    "beta_branched = {aa: (1 if aa in ('V','I','T') else 0) for aa in kd_hydro.keys()}\n",
    "\n",
    "aa_properties = {}\n",
    "canonical_set = set(kd_hydro.keys())\n",
    "for aa in canonical_set:\n",
    "    hydro_norm  = (kd_hydro[aa] + 4.5) / 9.0\n",
    "    volume_norm = sidechain_volume[aa] / 227.8\n",
    "    pol_norm    = (polarity[aa] - 4.9) / (13.0 - 4.9)\n",
    "    helix_norm  = choufa_helix[aa] / 1.51\n",
    "    sheet_norm  = choufa_sheet[aa] / 1.70\n",
    "    asa_norm    = (rel_ASA[aa] - 0.54) / (1.48 - 0.54)\n",
    "    aromatic    = 1 if aa in ('F','Y','W') else 0\n",
    "\n",
    "    aa_properties[aa] = [\n",
    "        hydro_norm,          # [0]\n",
    "        charge[aa],          # [1]\n",
    "        h_donors[aa],        # [2]\n",
    "        h_acceptors[aa],     # [3]\n",
    "        flexibility[aa],     # [4]\n",
    "        volume_norm,         # [5]\n",
    "        pol_norm,            # [6]\n",
    "        aromatic,            # [7]\n",
    "        helix_norm,          # [8]\n",
    "        sheet_norm,          # [9]\n",
    "        asa_norm,            # [10]\n",
    "        beta_branched[aa]    # [11]\n",
    "    ]\n",
    "\n",
    "# ─── (B) Load FASTA sequences ─────────────────────────────────────────────────\n",
    "def load_fasta(filepath, filter_non_canonical=False):\n",
    "    seqs = []\n",
    "    try:\n",
    "        with open(filepath) as f:\n",
    "            header = None\n",
    "            seq = \"\"\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\">\"):\n",
    "                    if header is not None and seq:\n",
    "                        if (not filter_non_canonical) or (set(seq) <= canonical_set):\n",
    "                            seqs.append(seq)\n",
    "                    header = line\n",
    "                    seq = \"\"\n",
    "                else:\n",
    "                    seq += line\n",
    "            if header is not None and seq: # Add the last sequence\n",
    "                if (not filter_non_canonical) or (set(seq) <= canonical_set):\n",
    "                    seqs.append(seq)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File not found {filepath}. Returning empty list.\")\n",
    "    return seqs\n",
    "\n",
    "# Ensure these files exist in the same directory as your script,\n",
    "# or provide full paths.\n",
    "pdb_seqs    = load_fasta(\"pdb_chains.fasta\",   filter_non_canonical=False)\n",
    "disprot_seqs = load_fasta(\"disprot_13000.fasta\", filter_non_canonical=False)\n",
    "\n",
    "print(f\"Loaded {len(pdb_seqs)} PDB sequences.\")\n",
    "print(f\"Loaded {len(disprot_seqs)} DisProt sequences.\")\n",
    "\n",
    "if not pdb_seqs and not disprot_seqs:\n",
    "    print(\"Error: No sequences loaded. Exiting.\")\n",
    "    exit()\n",
    "elif not pdb_seqs:\n",
    "    print(\"Warning: No PDB sequences loaded. Classifier might not be meaningful.\")\n",
    "elif not disprot_seqs:\n",
    "    print(\"Warning: No DisProt sequences loaded. Classifier might not be meaningful.\")\n",
    "\n",
    "\n",
    "# ─── (C) Compute each chain’s 7 global features ────────────────────────────────\n",
    "def compute_global_features(sequence):\n",
    "    props = []\n",
    "    # Filter out non-canonical amino acids from the sequence before processing\n",
    "    # to avoid KeyError if a non-canonical AA is encountered.\n",
    "    # However, the current aa_properties dictionary is built only from canonical_set,\n",
    "    # so this check is implicitly handled by `if aa in aa_properties:`.\n",
    "    # If sequences can contain 'X', 'U', 'O', etc., more robust filtering might be needed\n",
    "    # or aa_properties expanded.\n",
    "    valid_aas_in_sequence = 0\n",
    "    for aa in sequence:\n",
    "        if aa in aa_properties:\n",
    "            v = aa_properties[aa]\n",
    "            props.append([\n",
    "                v[0],               # hydrophobicity_norm\n",
    "                v[1],               # charge\n",
    "                v[2] + v[3],        # h_dh_a (sum of H-bond donors and acceptors)\n",
    "                v[4] / 0.544,       # norm_flex (flexibility normalized by G's flexibility)\n",
    "                v[6],               # pol_norm (normalized polarity)\n",
    "                v[7] + v[8],        # arom_plus_helix (sum of aromatic indicator and helix propensity)\n",
    "                v[10]               # asa_norm (normalized relative solvent accessibility)\n",
    "            ])\n",
    "            valid_aas_in_sequence +=1\n",
    "    \n",
    "    if not props or valid_aas_in_sequence == 0: # Handle empty sequences or sequences with no canonical AAs\n",
    "        return np.zeros(7) # Return a vector of zeros if no properties could be computed\n",
    "    return np.mean(np.vstack(props), axis=0)\n",
    "\n",
    "all_features_list = [] # Changed name to avoid conflict with pandas df\n",
    "all_labels_list   = [] # Changed name\n",
    "\n",
    "for seq in pdb_seqs:\n",
    "    # Skip empty sequences if any\n",
    "    if seq: \n",
    "        all_features_list.append(compute_global_features(seq))\n",
    "        all_labels_list.append(1)   # 1 = folded (PDB)\n",
    "\n",
    "for seq in disprot_seqs:\n",
    "    # Skip empty sequences if any\n",
    "    if seq:\n",
    "        all_features_list.append(compute_global_features(seq))\n",
    "        all_labels_list.append(0)   # 0 = disordered (DisProt)\n",
    "\n",
    "if not all_features_list:\n",
    "    print(\"Error: No features could be computed from the loaded sequences. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "df_all_data = pd.DataFrame(\n",
    "    all_features_list,\n",
    "    columns=[\n",
    "        \"hydro_norm\",\n",
    "        \"charge\",\n",
    "        \"h_dh_a\",\n",
    "        \"norm_flex\",\n",
    "        \"pol_norm\",\n",
    "        \"arom_plus_helix\",\n",
    "        \"asa_norm\"\n",
    "    ])\n",
    "df_all_data[\"label\"] = all_labels_list\n",
    "\n",
    "# ─── (D) Split data into Training and Testing sets ────────────────────────────\n",
    "# Ensure there's enough data to split, especially for stratification\n",
    "if df_all_data['label'].nunique() < 2:\n",
    "    print(\"Error: Need at least two classes for stratification. Exiting.\")\n",
    "    # Or handle differently, e.g., proceed without stratification if only one class loaded\n",
    "    # For now, exiting as classification is not meaningful.\n",
    "    if not df_all_data.empty:\n",
    "         print(df_all_data['label'].value_counts())\n",
    "    exit()\n",
    "    \n",
    "# Check if each class has enough members for the split\n",
    "min_class_count = df_all_data['label'].value_counts().min()\n",
    "if min_class_count < 2 and len(df_all_data) > 1 : # n_splits for StratifiedKFold is 2 by default for test_size > 0\n",
    "    print(f\"Warning: The smallest class has only {min_class_count} member(s), which might be too few for stratified splitting depending on test_size.\")\n",
    "    # Proceeding with splitting, but be aware of potential issues if test_size is too large relative to min_class_count\n",
    "    # If min_class_count is 1, stratification will fail.\n",
    "    if min_class_count == 1:\n",
    "        print(\"Error: Smallest class has only 1 member. Stratification requires at least 2 members per class. Consider non-stratified split or getting more data.\")\n",
    "        # As a fallback, could do a non-stratified split if user desires, but for now, exiting.\n",
    "        exit()\n",
    "\n",
    "\n",
    "# Use a test_size, e.g., 0.2 for 20% test data\n",
    "# random_state for reproducibility\n",
    "# stratify by labels to maintain class proportions in train and test sets\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_all_data.drop(columns=[\"label\"]),\n",
    "        df_all_data[\"label\"],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df_all_data[\"label\"] # Stratify if both classes are present\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Error during train_test_split, possibly due to insufficient samples in a class for stratification: {e}\")\n",
    "    print(\"Consider using a smaller test_size or ensuring more samples per class.\")\n",
    "    # Fallback: try without stratification if error is due to it and min_class_count was an issue\n",
    "    # For now, exiting.\n",
    "    exit()\n",
    "\n",
    "\n",
    "df_train = X_train.copy()\n",
    "df_train[\"label\"] = y_train\n",
    "\n",
    "df_test = X_test.copy()\n",
    "df_test[\"label\"] = y_test\n",
    "\n",
    "print(f\"\\nTraining set size: {len(df_train)}\")\n",
    "print(f\"Testing set size: {len(df_test)}\")\n",
    "print(f\"Training set PDB (1) count: {y_train.sum()}, DisProt (0) count: {len(y_train) - y_train.sum()}\")\n",
    "print(f\"Testing set PDB (1) count: {y_test.sum()}, DisProt (0) count: {len(y_test) - y_test.sum()}\")\n",
    "\n",
    "\n",
    "# ─── (E) Compute midpoint thresholds (mean of PDB vs. mean of DisProt) ON TRAINING DATA ONLY ───\n",
    "# Check if both labels are present in the training set after split\n",
    "if df_train[\"label\"].nunique() < 2:\n",
    "    print(\"\\nError: Training set does not contain both classes after split. Cannot compute midpoints.\")\n",
    "    # This can happen if one class had very few samples and all went to the test set (unlikely with stratification but possible with tiny datasets)\n",
    "    # Or if the initial dataset was highly imbalanced and very small.\n",
    "    print(df_train[\"label\"].value_counts())\n",
    "    midpoints = {col: 0.5 for col in X_train.columns} # Default midpoints if calculation fails\n",
    "    print(\"Warning: Using default midpoints (0.5) as training data lacks class diversity.\")\n",
    "else:\n",
    "    train_means = df_train.groupby(\"label\").mean().rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "    # Ensure both \"PDB\" and \"DisProt\" indices exist in train_means before calculating midpoints\n",
    "    if \"PDB\" not in train_means.index or \"DisProt\" not in train_means.index:\n",
    "        print(\"\\nError: Could not find means for both PDB and DisProt in the training set.\")\n",
    "        print(train_means)\n",
    "        midpoints = {col: 0.5 for col in X_train.columns} # Default midpoints\n",
    "        print(\"Warning: Using default midpoints (0.5) as PDB/DisProt means are missing in training data.\")\n",
    "    else:\n",
    "        midpoints = {col: (train_means.loc[\"PDB\", col] + train_means.loc[\"DisProt\", col]) / 2\n",
    "                     for col in X_train.columns} # Iterate over X_train.columns (feature columns)\n",
    "\n",
    "        print(\"\\nGlobal Feature Means (DisProt vs. PDB) from TRAINING DATA:\\n\")\n",
    "        print(train_means, \"\\n\")\n",
    "        print(\"Chosen Midpoint Thresholds (from TRAINING DATA):\\n\")\n",
    "        for feat, t in midpoints.items():\n",
    "            print(f\"  {feat:18s} = {t:.3f}\")\n",
    "        print()\n",
    "\n",
    "# ─── (F) Count how many of the 7 conditions each chain in the TEST SET satisfies ───────────────\n",
    "# This function uses the 'midpoints' dictionary calculated from the training data\n",
    "def count_conditions_test(row, midpoints_dict):\n",
    "    c1 = row[\"hydro_norm\"]          >= midpoints_dict[\"hydro_norm\"]\n",
    "    c2 = abs(row[\"charge\"])         <= abs(midpoints_dict[\"charge\"]) # abs for midpoint too, if it could be negative\n",
    "    c3 = row[\"h_dh_a\"]              <= midpoints_dict[\"h_dh_a\"]\n",
    "    c4 = row[\"norm_flex\"]           <= midpoints_dict[\"norm_flex\"]\n",
    "    c5 = row[\"pol_norm\"]            <= midpoints_dict[\"pol_norm\"]\n",
    "    c6 = row[\"arom_plus_helix\"]     >= midpoints_dict[\"arom_plus_helix\"]\n",
    "    c7 = row[\"asa_norm\"]            <= midpoints_dict[\"asa_norm\"]\n",
    "    return sum([c1, c2, c3, c4, c5, c6, c7])\n",
    "\n",
    "# Apply to the test set\n",
    "df_test[\"conditions_met\"] = df_test.apply(lambda row: count_conditions_test(row, midpoints), axis=1)\n",
    "\n",
    "\n",
    "# Show the distribution of “conditions_met” separately for PDB vs. DisProt in the TEST SET\n",
    "if not df_test.empty:\n",
    "    # Ensure 'label' and 'conditions_met' are in df_test\n",
    "    if 'label' in df_test.columns and 'conditions_met' in df_test.columns:\n",
    "        dist_test = df_test.groupby(\"label\")[\"conditions_met\"] \\\n",
    "                      .value_counts() \\\n",
    "                      .unstack(fill_value=0) \\\n",
    "                      .rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "        pd.set_option(\"display.max_columns\", None)\n",
    "        print(\"Distribution of ‘conditions_met’ by Label (ON TEST SET):\\n\")\n",
    "        print(dist_test, \"\\n\")\n",
    "    else:\n",
    "        print(\"Warning: 'label' or 'conditions_met' column missing in df_test for distribution display.\")\n",
    "else:\n",
    "    print(\"Warning: Test set is empty. Cannot show distribution of 'conditions_met'.\")\n",
    "\n",
    "\n",
    "# ─── (G) For each k=1…7, classify “folded if conditions_met ≥ k” ON TEST SET ─────────────\n",
    "results = []\n",
    "if not df_test.empty and 'conditions_met' in df_test.columns and 'label' in df_test.columns :\n",
    "    for k_threshold in range(1, 8): # Renamed k to k_threshold to avoid conflict\n",
    "        # Predictions on the test set\n",
    "        preds_test = (df_test[\"conditions_met\"] >= k_threshold).astype(int)\n",
    "        \n",
    "        # True labels from the test set\n",
    "        true_test_labels = df_test[\"label\"]\n",
    "        \n",
    "        tp = ((preds_test == 1) & (true_test_labels == 1)).sum()\n",
    "        fn = ((preds_test == 0) & (true_test_labels == 1)).sum()\n",
    "        tn = ((preds_test == 0) & (true_test_labels == 0)).sum()\n",
    "        fp = ((preds_test == 1) & (true_test_labels == 0)).sum()\n",
    "        \n",
    "        # Calculate accuracy, precision, recall, f1-score for more complete evaluation\n",
    "        # Avoid division by zero if a class is not present or no predictions for a class\n",
    "        total_samples_test = len(df_test)\n",
    "        acc = (tp + tn) / total_samples_test if total_samples_test > 0 else 0\n",
    "        \n",
    "        precision_pdb = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall_pdb = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_pdb = 2 * (precision_pdb * recall_pdb) / (precision_pdb + recall_pdb) if (precision_pdb + recall_pdb) > 0 else 0\n",
    "\n",
    "        precision_disprot = tn / (tn + fn) if (tn + fn) > 0 else 0 # Note: this is not standard precision for class 0\n",
    "        recall_disprot = tn / (tn + fp) if (tn + fp) > 0 else 0 # Specificity for class 1\n",
    "        \n",
    "        results.append({\n",
    "            \"k (min # of features)\": k_threshold,\n",
    "            \"TP\": tp, \"FN\": fn, \"TN\": tn, \"FP\": fp,\n",
    "            \"Accuracy\": f\"{acc:.2%}\",\n",
    "            \"Precision (PDB)\": f\"{precision_pdb:.2%}\",\n",
    "            \"Recall (PDB)\": f\"{recall_pdb:.2%}\",\n",
    "            \"F1-score (PDB)\": f\"{f1_pdb:.2%}\"\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "    # pd.set_option(\"display.max_columns\", None) # Already set\n",
    "    print(\"Performance on TEST SET as we vary k = minimum # of satisfied conditions:\\n\")\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "    # For a more standard report:\n",
    "    print(\"\\n--- Detailed Classification Report for best k (example: k=4, inspect df_results) ---\")\n",
    "    # Example: Find best k by F1-score for PDB or overall accuracy\n",
    "    if not df_results.empty:\n",
    "        # Convert F1-score (PDB) from string percentage to float for finding max\n",
    "        try:\n",
    "            df_results['F1_PDB_float'] = df_results['F1-score (PDB)'].str.rstrip('%').astype('float') / 100.0\n",
    "            best_k_row = df_results.loc[df_results['F1_PDB_float'].idxmax()]\n",
    "            best_k = int(best_k_row[\"k (min # of features)\"])\n",
    "            print(f\"Best k based on F1-score (PDB) is: {best_k}\")\n",
    "            \n",
    "            best_preds_test = (df_test[\"conditions_met\"] >= best_k).astype(int)\n",
    "            print(classification_report(true_test_labels, best_preds_test, target_names=[\"DisProt (0)\", \"PDB (1)\"]))\n",
    "\n",
    "            cm = confusion_matrix(true_test_labels, best_preds_test)\n",
    "            cm_df = pd.DataFrame(cm, index=[\"Actual DisProt\",\"Actual PDB\"], columns=[\"Pred DisProt\",\"Pred PDB\"])\n",
    "            print(\"Confusion Matrix for best k:\\n\", cm_df)\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Could not determine best k automatically, F1_PDB_float column missing.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error determining best k: {e}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Test set is empty or 'conditions_met'/'label' columns are missing. Cannot evaluate performance.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c89ef6-d36f-420a-9f5b-8f796a7a635f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
