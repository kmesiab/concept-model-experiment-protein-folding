{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb269f7-6795-42aa-8b2b-13b6e972d16b",
   "metadata": {},
   "source": [
    "## Explanation of the Protein Folding Prediction Script\n",
    "\n",
    "This Python script is designed to classify protein sequences as either \"Folded\" (typically from PDB) or \"Disordered\" (typically from DisProt). It does this by:\n",
    "1.  Defining a set of biophysical and compositional features for amino acid sequences.\n",
    "2.  Implementing and evaluating two main rule-based classification approaches.\n",
    "3.  Using a proper training/testing split to ensure fair evaluation of the classifiers.\n",
    "\n",
    "The script's structure and logic can be mapped to the **Concept Model** framework (M1: Properties, M2: Constraints, M3: Transformations, M4: Goal State).\n",
    "\n",
    "### Key Components of the Script:\n",
    "\n",
    "1.  **Amino Acid Property Definitions (`aa_properties_base`):**\n",
    "    * The script begins by defining various fundamental physicochemical properties for each of the 20 canonical amino acids (e.g., hydrophobicity, charge, flexibility, propensity for helix/sheet).\n",
    "    * These base properties are normalized and stored. They are the building blocks for the features used by the classifiers.\n",
    "\n",
    "2.  **Data Loading (`load_fasta_with_labels`):**\n",
    "    * Protein sequences are loaded from FASTA files (`pdb_chains.fasta` for folded, `disprot_13000.fasta` for disordered).\n",
    "    * Each sequence is stored along with its true label (1 for PDB/Folded, 0 for DisProt/Disordered) and its raw sequence string.\n",
    "\n",
    "3.  **New 7 Feature Definitions (`compute_new_seven_features`):**\n",
    "    * A function `compute_new_seven_features` is defined to calculate a specific set of 7 features for any given sequence string (which could be a whole protein or a shorter window/segment). These features are:\n",
    "        1.  `hydro_norm_avg`: Average normalized hydrophobicity.\n",
    "        2.  `flex_norm_avg`: Average normalized flexibility.\n",
    "        3.  `h_bond_potential_avg`: Average H-bonding potential (sum of donors/acceptors).\n",
    "        4.  `abs_net_charge_prop`: Absolute proportion of net charge.\n",
    "        5.  `shannon_entropy`: A measure of sequence complexity.\n",
    "        6.  `freq_proline`: Frequency of Proline.\n",
    "        7.  `freq_bulky_hydrophobics`: Combined frequency of W, C, F, Y, I, V, L.\n",
    "    * **Concept Model M1 (Property Vectors / Tensor Snapshots):** This set of 7 features calculated for a protein (or segment) constitutes its M1 representation – a vector of its key properties.\n",
    "\n",
    "4.  **Main Feature Computation (`compute_features_for_dataset`):**\n",
    "    * This function processes a list of raw sequences.\n",
    "    * It can either calculate the 7 new features globally for each entire protein (if `WINDOW_SIZE_BASELINE` is `None`) or calculate them for sliding windows and then average these window features to get 7 global values for the protein. For the \"New Global Features Classifier\" part, it's set to compute direct global features.\n",
    "\n",
    "5.  **Train/Test Split:**\n",
    "    * The full dataset (with globally computed new features and labels) is split into a training set (80%) and a testing set (20%). This is crucial for an unbiased evaluation of how well the classifiers generalize to unseen data.\n",
    "    * Raw sequences corresponding to the test set are kept aside for the sliding window classifier.\n",
    "\n",
    "6.  **Midpoint Calculation (from Training Data's Global Features):**\n",
    "    * From the **training set's global features**, the script calculates the average value of each of the 7 new features for PDB proteins and for DisProt proteins.\n",
    "    * The `midpoints` are then calculated as the halfway point between these PDB and DisProt averages for each feature.\n",
    "    * **Concept Model M2 (Constraints):** These empirically derived `midpoints` define the thresholds for the classification rules. A condition like `feature_value >= midpoint` (or `<= midpoint`, depending on the feature) acts as a constraint. A protein/segment feature vector is tested against these constraints.\n",
    "\n",
    "7.  **Defining \"Conditions Met\" (`count_conditions_for_new_feature_vector`):**\n",
    "    * This helper function takes a 7-feature vector (for a protein or a segment) and the `midpoints`.\n",
    "    * It checks, for each of the 7 features, whether it falls on the \"PDB-like\" side of its respective midpoint (e.g., higher hydrophobicity, lower proline frequency). The direction of comparison (`>=` or `<=`) is determined by observing the means of PDB vs. DisProt proteins in the training data.\n",
    "    * It returns the total number of conditions (out of 7) that were met.\n",
    "\n",
    "### Classifier 1: Baseline Threshold-Based Classifier (New Global Features)\n",
    "\n",
    "* **Logic:**\n",
    "    1.  The 7 new global features are calculated for each protein in the test set.\n",
    "    2.  For each test protein, `count_conditions_for_new_feature_vector` determines how many of its 7 global features satisfy the midpoint-derived conditions. This result is stored as `conditions_met`.\n",
    "    3.  The script then evaluates performance by trying different thresholds `k` (from 1 to 7). A protein is predicted as \"Folded\" if its `conditions_met >= k`.\n",
    "* **Relation to Concept Model:**\n",
    "    * **M1:** The 7 new global features of an entire protein.\n",
    "    * **M2:** The set of 7 conditions derived from `midpoints`.\n",
    "    * **M3 (Transformation/Rule):** The process of (a) counting how many conditions are met by M1, and (b) comparing this count to a threshold `k`.\n",
    "    * **M4 (Goal State):** The true labels (Folded/Disordered). The script finds the `k` that yields the best F1-score for PDB proteins, effectively optimizing this simple M3 rule against M4.\n",
    "\n",
    "### Classifier 2: Sliding Window (Larger - 9 AA) Classifier with Failure Cancellation\n",
    "\n",
    "* **Logic:** This classifier processes each raw protein sequence in the test set with a more complex, stateful rule:\n",
    "    1.  **Parameters:**\n",
    "        * `SLIDING_WINDOW_SIZE = 9` (each local window to analyze).\n",
    "        * `SLIDING_WINDOW_SLIDE_STEP = 9` (non-overlapping windows).\n",
    "        * `SLIDING_WINDOW_PASS_K = 4` (a 9-AA window \"passes\" if its 7 *local* features meet at least 4 conditions, judged by the *globally-derived `midpoints`*).\n",
    "        * `MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING = 3` (the protein is \"Folded\" if it has 3 or fewer uncancelled failed windows).\n",
    "    2.  **Serial Processing:** It slides a 9-AA window across the sequence.\n",
    "    3.  **Window Evaluation:** For each 9-AA window, its 7 new features are calculated. `count_conditions_for_new_feature_vector` determines if this window \"passes\" or \"fails\" based on `SLIDING_WINDOW_PASS_K` and the global `midpoints`.\n",
    "    4.  **Failure Cancellation:** A running count of `current_consecutive_failures_streak` is maintained. If a window \"passes,\" this streak is reset to 0 (any failures in that streak are \"cancelled\"). If a window \"fails,\" the streak count increases.\n",
    "    5.  **Protein Classification:** After all windows are processed, the `total_unforgiven_failures` is simply the value of `current_consecutive_failures_streak` at the end of the sequence (as any streaks terminated by a pass were reset). The protein is predicted \"Folded\" if `total_unforgiven_failures <= MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING`.\n",
    "* **Relation to Concept Model:**\n",
    "    * **M1 (local):** The 7 new features calculated for each 9-AA window.\n",
    "    * **M2 (local):** The conditions a window must meet (based on global `midpoints` and `SLIDING_WINDOW_PASS_K`) to \"pass.\"\n",
    "    * **M3 (Transformation/Rule):** This is a more complex M3. It involves:\n",
    "        * The serial processing of windows.\n",
    "        * The evaluation of each window against local M2.\n",
    "        * The stateful tracking of `current_consecutive_failures_streak`.\n",
    "        * The \"failure cancellation\" logic.\n",
    "        * The final decision rule based on `total_unforgiven_failures` and `MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING`.\n",
    "    * **M4 (Goal State):** The true labels (Folded/Disordered) that this entire M3 rule system is trying to predict.\n",
    "\n",
    "### Summary\n",
    "The script first establishes a baseline performance using a threshold classifier on 7 new global features. It then tests a more intricate, serial window-based classifier with a failure cancellation mechanism, using the same underlying feature definitions (calculated locally) and the same globally-derived midpoints for local window evaluation. The results then show how these different approaches (different M1 aggregations and different M3 rules) perform at predicting the M4 goal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b534985e-b516-41af-b343-14da11c248bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Extracted 15000 chains → 'pdb_chains.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 1.) Download the PDB chain sequences (FASTA format from RCSB) via the HTTPS mirror,\n",
    "#      then keep only the first 15 000 entries.\n",
    "\n",
    "import requests\n",
    "\n",
    "# Use the “files.wwpdb.org” HTTPS mirror instead of FTP\n",
    "pdb_url = \"https://files.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt\"\n",
    "try:\n",
    "    resp = requests.get(pdb_url, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text.strip()\n",
    "    if not text.startswith(\">\"):\n",
    "        raise RuntimeError(\"Downloaded content does not look like FASTA.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to download PDB chain sequences: {e}\")\n",
    "\n",
    "# Write the complete dump to a temporary file\n",
    "with open(\"pdb_chains.fasta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "# ─── Split the full FASTA into individual (header, sequence) tuples ─────────────\n",
    "def split_fasta(filepath):\n",
    "    sequences = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        header = None\n",
    "        seq_lines = []\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None:\n",
    "                    sequences.append((header, \"\".join(seq_lines)))\n",
    "                header = line\n",
    "                seq_lines = []\n",
    "            else:\n",
    "                seq_lines.append(line)\n",
    "        # Add the final sequence\n",
    "        if header is not None:\n",
    "            sequences.append((header, \"\".join(seq_lines)))\n",
    "    return sequences\n",
    "\n",
    "all_chains = split_fasta(\"pdb_chains.fasta\")\n",
    "\n",
    "# ─── Keep exactly the first 15 000 chains ─────────────────────────────────────────\n",
    "subset = all_chains[:15000]\n",
    "\n",
    "# ─── Write those 15 000 chains back to “pdb_chains.fasta” ───────────────────────\n",
    "with open(\"pdb_chains.fasta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for header, seq in subset:\n",
    "        f.write(f\"{header}\\n\")\n",
    "        f.write(f\"{seq}\\n\")\n",
    "\n",
    "print(f\"✔ Extracted {len(subset)} chains → 'pdb_chains.fasta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e30de29-e8f5-4fe7-abfd-58594dc600e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Successfully fetched 100 DisProt sequences in FASTA format → 'disprot_1000.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 2.) Use DisProt’s search endpoint with format=fasta\n",
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://disprot.org/api/search?format=fasta&limit=10000\"\n",
    "try:\n",
    "    resp = requests.get(url, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to GET DisProt FASTA via API: {e}\")\n",
    "\n",
    "text = resp.text.strip()\n",
    "\n",
    "# 2.2) Quick sanity check: FASTA must start with '>', not '<'\n",
    "if not text.startswith(\">\"):\n",
    "    raise RuntimeError(\n",
    "        \"Downloaded content does not look like FASTA. \"\n",
    "        \"If it begins with '<', you're still hitting an HTML page instead of raw FASTA.\"\n",
    "    )\n",
    "\n",
    "# 2.3) Write the 100 DisProt entries to a file\n",
    "with open(\"disprot_13000.fasta\", \"w\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "print(\"✔ Successfully fetched 100 DisProt sequences in FASTA format → 'disprot_1000.fasta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9b660b-dd1e-48ac-a9d2-8dc830cae3d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Fetched 25000 DisProt sequences → 'disprot_13000.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 2.1) Collect more data\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# ─── PARAMETERS ─────────────────────────────────────────────────────────────\n",
    "TOTAL_DESIRED = 25_000   # how many DisProt sequences we want total\n",
    "PER_PAGE      = 100      # DisProt’s hard cap per request\n",
    "OUTPUT_FILE   = \"disprot_13000.fasta\"\n",
    "\n",
    "accum_seqs = []\n",
    "offset     = 0\n",
    "\n",
    "while len(accum_seqs) < TOTAL_DESIRED:\n",
    "    url = f\"https://disprot.org/api/search?format=fasta&limit={PER_PAGE}&offset={offset}\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to GET DisProt FASTA (offset={offset}): {e}\")\n",
    "\n",
    "    block = resp.text.strip()\n",
    "    if not block.startswith(\">\"):\n",
    "        raise RuntimeError(\n",
    "            \"Downloaded content does not look like FASTA. \"\n",
    "            \"If it begins with '<', you're still hitting an HTML page.\"\n",
    "        )\n",
    "\n",
    "    # Parse out this page’s FASTA sequences (collecting only the raw sequences, not full headers):\n",
    "    raw_lines = block.splitlines()\n",
    "    header = None\n",
    "    seq_buf = \"\"\n",
    "    this_page_seqs = []\n",
    "    for line in raw_lines:\n",
    "        if line.startswith(\">\"):\n",
    "            if header is not None and seq_buf:\n",
    "                this_page_seqs.append(seq_buf)\n",
    "            header = line\n",
    "            seq_buf = \"\"\n",
    "        else:\n",
    "            seq_buf += line.strip()\n",
    "    if header is not None and seq_buf:\n",
    "        this_page_seqs.append(seq_buf)\n",
    "\n",
    "    if not this_page_seqs:\n",
    "        # No more sequences returned → break out early\n",
    "        break\n",
    "\n",
    "    accum_seqs.extend(this_page_seqs)\n",
    "    offset += PER_PAGE\n",
    "\n",
    "    # Sleep briefly (so we don’t hammer the server)\n",
    "    time.sleep(0.4)\n",
    "\n",
    "# Trim in case we overshot\n",
    "accum_seqs = accum_seqs[:TOTAL_DESIRED]\n",
    "\n",
    "# Write out ~25k sequences in FASTA format (with minimal headers)\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    for i, seq in enumerate(accum_seqs):\n",
    "        f.write(f\">disprot_sequence_{i+1}\\n\")\n",
    "        f.write(seq + \"\\n\")\n",
    "\n",
    "print(f\"✔ Fetched {len(accum_seqs)} DisProt sequences → '{OUTPUT_FILE}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4177fdf8-80da-43a8-8fa1-2633860ed0cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">disprot_sequence_1\n",
      "EHVIEMDVTSENGQRALKEQSSKAKIVKNRWGRNVVQISNT\n",
      ">disprot_sequence_2\n",
      "VYRNSRAQGGG\n",
      ">disprot_sequence_3\n"
     ]
    }
   ],
   "source": [
    "# 2.2) Verify Downloaded Sequences\n",
    "with open(\"disprot_13000.fasta\") as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline().rstrip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adffd98f-0799-4b9e-9883-c790a96efe60",
   "metadata": {},
   "source": [
    "# Sliding Window vs Global Learned Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a75ce8de-d866-4859-861f-81bc69fef95e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15000 PDB sequences.\n",
      "Loaded 25000 DisProt sequences.\n",
      "\n",
      "Computing NEW GLOBAL features (WINDOW_SIZE_GLOBAL_FEATURES = None)...\n",
      "NEW GLOBAL feature computation complete.\n",
      "\n",
      "Global Features Training set size: 32000\n",
      "Global Features Testing set size: 8000\n",
      "\n",
      "Global Feature Means (DisProt vs. PDB) from NEW GLOBAL FEATURES TRAINING DATA:\n",
      "\n",
      "         hydro_norm_avg  flex_norm_avg  h_bond_potential_avg  \\\n",
      "label                                                          \n",
      "DisProt        0.401099       0.837249              1.256680   \n",
      "PDB            0.475068       0.806553              1.073287   \n",
      "\n",
      "         abs_net_charge_prop  shannon_entropy  freq_proline  \\\n",
      "label                                                         \n",
      "DisProt             0.115245         3.359850      0.069069   \n",
      "PDB                 0.036236         3.700737      0.043093   \n",
      "\n",
      "         freq_bulky_hydrophobics  \n",
      "label                             \n",
      "DisProt                 0.210505  \n",
      "PDB                     0.312975   \n",
      "\n",
      "Chosen Midpoint Thresholds (from NEW GLOBAL FEATURES TRAINING DATA):\n",
      "\n",
      "  hydro_norm_avg     = 0.438\n",
      "  flex_norm_avg      = 0.822\n",
      "  h_bond_potential_avg = 1.165\n",
      "  abs_net_charge_prop = 0.076\n",
      "  shannon_entropy    = 3.530\n",
      "  freq_proline       = 0.056\n",
      "  freq_bulky_hydrophobics = 0.262\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating New Global Features Threshold-Based Classifier ---\n",
      "Distribution of ‘conditions_met’ (NEW GLOBAL features) by Label (ON TEST SET):\n",
      "\n",
      "conditions_met    0     1     2     3    4    5     6    7\n",
      "label                                                     \n",
      "DisProt         299  1014  1205  1034  716  469   209   54\n",
      "PDB               6    28    49    90  333  563  1074  857 \n",
      "\n",
      "Performance of New Global Features Classifier on TEST SET (varying k):\n",
      "\n",
      " k   TP   FN   TN   FP Accuracy Precision (PDB) Recall (PDB) F1-score (PDB)\n",
      " 1 2994    6  299 4701   41.16%          38.91%       99.80%         55.99%\n",
      " 2 2966   34 1313 3687   53.49%          44.58%       98.87%         61.45%\n",
      " 3 2917   83 2518 2482   67.94%          54.03%       97.23%         69.46%\n",
      " 4 2827  173 3552 1448   79.74%          66.13%       94.23%         77.72%\n",
      " 5 2494  506 4268  732   84.52%          77.31%       83.13%         80.12%\n",
      " 6 1931 1069 4737  263   83.35%          88.01%       64.37%         74.36%\n",
      " 7  857 2143 4946   54   72.54%          94.07%       28.57%         43.83%\n",
      "\n",
      "--- Detailed New Global Features Classification Report for best k = 5 (based on F1 PDB) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " DisProt (0)       0.89      0.85      0.87      5000\n",
      "     PDB (1)       0.77      0.83      0.80      3000\n",
      "\n",
      "    accuracy                           0.85      8000\n",
      "   macro avg       0.83      0.84      0.84      8000\n",
      "weighted avg       0.85      0.85      0.85      8000\n",
      "\n",
      "Confusion Matrix for best k (New Global Features):\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt          4268       732\n",
      "Actual PDB               506      2494\n",
      "\n",
      "\n",
      "--- Testing Sliding Window (Larger) Classifier with Failure Cancellation ---\n",
      "\n",
      "Applying Sliding Window (Larger) Classifier (NEW features) with Failure Cancellation: window_size=9, slide_step=9, window_k_pass_thresh=4, max_total_unforgiven_failures=3...\n",
      "\n",
      "Sliding Window (Larger) with Failure Cancellation classification complete.\n",
      "\n",
      "Performance of Sliding Window (Larger) Classifier with Failure Cancellation (ON TEST SET):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " DisProt (0)       0.86      0.18      0.30      5000\n",
      "     PDB (1)       0.41      0.95      0.57      3000\n",
      "\n",
      "    accuracy                           0.47      8000\n",
      "   macro avg       0.64      0.57      0.44      8000\n",
      "weighted avg       0.69      0.47      0.40      8000\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt           910      4090\n",
      "Actual PDB               149      2851\n",
      "Accuracy: 47.01%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "# --- Parameters for Classifiers ---\n",
    "# For Global Feature Classifier (WINDOW_SIZE_GLOBAL_FEATURES = None means direct global calculation)\n",
    "WINDOW_SIZE_GLOBAL_FEATURES = None \n",
    "\n",
    "# For Sliding Window Classifier\n",
    "SLIDING_WINDOW_SIZE = 9  # Size of the sliding window\n",
    "SLIDING_WINDOW_SLIDE_STEP = 9 # Step for sliding (equal to WINDOW_SIZE for non-overlapping)\n",
    "SLIDING_WINDOW_PASS_K = 4     # Min conditions a window must meet to \"pass\"\n",
    "MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING = 3 # Max uncancelled failed windows for protein to pass\n",
    "\n",
    "# ─── (A) Build aa_properties (underlying single AA properties) ────────────────\n",
    "kd_hydro = {\n",
    "    'A':  1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C':  2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I':  4.5,\n",
    "    'L':  3.8, 'K': -3.9, 'M':  1.9, 'F':  2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V':  4.2\n",
    "}\n",
    "charge = { # Simplified charge for H, assuming neutral pH for general calculation\n",
    "    'A':  0, 'R':  1, 'N':  0, 'D': -1, 'C':  0,\n",
    "    'Q':  0, 'E': -1, 'G':  0, 'H':  0, 'I':  0, \n",
    "    'L':  0, 'K':  1, 'M':  0, 'F':  0, 'P':  0,\n",
    "    'S':  0, 'T':  0, 'W':  0, 'Y':  0, 'V':  0\n",
    "}\n",
    "h_donors = {'A':0,'R':2,'N':2,'D':0,'C':0,'Q':2,'E':0,'G':0,'H':1,'I':0,\n",
    "            'L':0,'K':1,'M':0,'F':0,'P':0,'S':1,'T':1,'W':1,'Y':1,'V':0}\n",
    "h_acceptors = {'A':0,'R':0,'N':2,'D':2,'C':1,'Q':2,'E':2,'G':0,'H':1,'I':0,\n",
    "               'L':0,'K':0,'M':0,'F':0,'P':0,'S':1,'T':1,'W':0,'Y':1,'V':0}\n",
    "flexibility = {\n",
    "    'A': 0.357, 'R': 0.529, 'N': 0.463, 'D': 0.511, 'C': 0.346,\n",
    "    'Q': 0.493, 'E': 0.497, 'G': 0.544, 'H': 0.323, 'I': 0.462,\n",
    "    'L': 0.365, 'K': 0.466, 'M': 0.295, 'F': 0.314, 'P': 0.509,\n",
    "    'S': 0.507, 'T': 0.444, 'W': 0.305, 'Y': 0.420, 'V': 0.386\n",
    "}\n",
    "sidechain_volume = {\n",
    "    'A':  88.6, 'R': 173.4, 'N': 114.1, 'D': 111.1, 'C': 108.5, 'Q': 143.8, \n",
    "    'E': 138.4, 'G':  60.1, 'H': 153.2, 'I': 166.7, 'L': 166.7, 'K': 168.6, \n",
    "    'M': 162.9, 'F': 189.9, 'P': 112.7, 'S':  89.0, 'T': 116.1, 'W': 227.8, \n",
    "    'Y': 193.6, 'V': 140.0\n",
    "}\n",
    "polarity = {\n",
    "    'A':  8.1, 'R': 10.5, 'N': 11.6, 'D': 13.0, 'C':  5.5, 'Q': 10.5, \n",
    "    'E': 12.3, 'G':  9.0, 'H': 10.4, 'I':  5.2, 'L':  4.9, 'K': 11.3, \n",
    "    'M':  5.7, 'F':  5.2, 'P':  8.0, 'S':  9.2, 'T':  8.6, 'W':  5.4, \n",
    "    'Y':  6.2, 'V':  5.9\n",
    "}\n",
    "choufa_helix = {\n",
    "    'A': 1.45, 'R': 0.79, 'N': 0.73, 'D': 1.01, 'C': 0.77, 'Q': 1.17, \n",
    "    'E': 1.51, 'G': 0.53, 'H': 1.00, 'I': 1.08, 'L': 1.34, 'K': 1.07, \n",
    "    'M': 1.20, 'F': 1.12, 'P': 0.59, 'S': 0.79, 'T': 0.82, 'W': 1.14, \n",
    "    'Y': 0.61, 'V': 1.06\n",
    "}\n",
    "choufa_sheet = {\n",
    "    'A': 0.97, 'R': 0.90, 'N': 0.65, 'D': 0.54, 'C': 1.30, 'Q': 1.23, \n",
    "    'E': 0.37, 'G': 0.75, 'H': 0.87, 'I': 1.60, 'L': 1.22, 'K': 0.74, \n",
    "    'M': 1.67, 'F': 1.28, 'P': 0.62, 'S': 0.72, 'T': 1.20, 'W': 1.19, \n",
    "    'Y': 1.29, 'V': 1.70\n",
    "}\n",
    "rel_ASA = {\n",
    "    'A': 0.74, 'R': 1.48, 'N': 1.14, 'D': 1.23, 'C': 0.86, 'Q': 1.36, \n",
    "    'E': 1.26, 'G': 1.00, 'H': 0.91, 'I': 0.59, 'L': 0.61, 'K': 1.29, \n",
    "    'M': 0.64, 'F': 0.65, 'P': 0.71, 'S': 1.42, 'T': 1.20, 'W': 0.55, \n",
    "    'Y': 0.63, 'V': 0.54\n",
    "}\n",
    "beta_branched = {aa: (1 if aa in ('V','I','T') else 0) for aa in kd_hydro.keys()}\n",
    "\n",
    "aa_properties_base = {} \n",
    "canonical_set = set(kd_hydro.keys())\n",
    "for aa in canonical_set:\n",
    "    aa_properties_base[aa] = {\n",
    "        'hydro_norm': (kd_hydro[aa] + 4.5) / 9.0,\n",
    "        'charge_val': charge[aa], \n",
    "        'h_donors': h_donors[aa],\n",
    "        'h_acceptors': h_acceptors[aa],\n",
    "        'flexibility': flexibility[aa],\n",
    "        'volume_norm': sidechain_volume[aa] / 227.8,\n",
    "        'pol_norm': (polarity[aa] - 4.9) / (13.0 - 4.9),\n",
    "        'is_aromatic': 1 if aa in ('F','Y','W') else 0,\n",
    "        'helix_prop': choufa_helix[aa] / 1.51,\n",
    "        'sheet_prop': choufa_sheet[aa] / 1.70,\n",
    "        'asa_norm': (rel_ASA[aa] - 0.54) / (1.48 - 0.54),\n",
    "        'is_beta_branched': beta_branched[aa]\n",
    "    }\n",
    "\n",
    "# ─── (B) Load FASTA sequences & Store Raw Sequences with Labels ────────────────\n",
    "def load_fasta_with_labels(filepath, label, filter_non_canonical=False):\n",
    "    sequences_with_labels = []\n",
    "    try:\n",
    "        with open(filepath) as f:\n",
    "            header = None; seq_content = \"\"\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\">\"):\n",
    "                    if header is not None and seq_content:\n",
    "                        if (not filter_non_canonical) or (set(seq_content) <= canonical_set):\n",
    "                            sequences_with_labels.append({'sequence': seq_content, 'label': label, 'header': header})\n",
    "                    header = line; seq_content = \"\"\n",
    "                else: seq_content += line\n",
    "            if header is not None and seq_content: # Last sequence\n",
    "                if (not filter_non_canonical) or (set(seq_content) <= canonical_set):\n",
    "                    sequences_with_labels.append({'sequence': seq_content, 'label': label, 'header': header})\n",
    "    except FileNotFoundError: print(f\"Warning: File not found {filepath}.\")\n",
    "    return sequences_with_labels\n",
    "\n",
    "all_sequences_data = []\n",
    "all_sequences_data.extend(load_fasta_with_labels(\"pdb_chains.fasta\", 1))\n",
    "all_sequences_data.extend(load_fasta_with_labels(\"disprot_13000.fasta\", 0))\n",
    "\n",
    "print(f\"Loaded {len([item for item in all_sequences_data if item['label'] == 1])} PDB sequences.\")\n",
    "print(f\"Loaded {len([item for item in all_sequences_data if item['label'] == 0])} DisProt sequences.\")\n",
    "\n",
    "if not all_sequences_data:\n",
    "    print(\"Error: No sequences loaded. Exiting.\"); exit()\n",
    "\n",
    "# ─── (C) NEW Feature Computation Functions ───────────────────────────────────\n",
    "def get_aa_composition(sequence_str):\n",
    "    composition = {aa: 0 for aa in canonical_set}\n",
    "    valid_len = 0\n",
    "    for aa in sequence_str:\n",
    "        if aa in canonical_set:\n",
    "            composition[aa] += 1\n",
    "            valid_len += 1\n",
    "    if valid_len == 0: return {aa: 0.0 for aa in canonical_set}, 0\n",
    "    for aa in composition: composition[aa] /= valid_len\n",
    "    return composition, valid_len\n",
    "\n",
    "def calculate_shannon_entropy(aa_composition):\n",
    "    entropy = 0.0\n",
    "    for aa_freq in aa_composition.values(): # Iterate over frequencies directly\n",
    "        if aa_freq > 0:\n",
    "            entropy -= aa_freq * math.log2(aa_freq)\n",
    "    return entropy\n",
    "\n",
    "def compute_new_seven_features(sequence_str):\n",
    "    if not sequence_str: return np.zeros(7)\n",
    "    composition, valid_seq_len = get_aa_composition(sequence_str)\n",
    "    if valid_seq_len == 0: return np.zeros(7)\n",
    "\n",
    "    hydro_norm_sum, flex_norm_sum, h_bond_potential_sum = 0, 0, 0\n",
    "    for aa in sequence_str:\n",
    "        if aa in aa_properties_base:\n",
    "            props = aa_properties_base[aa]\n",
    "            hydro_norm_sum += props['hydro_norm']\n",
    "            flex_norm_sum += props['flexibility'] / 0.544\n",
    "            h_bond_potential_sum += props['h_donors'] + props['h_acceptors']\n",
    "\n",
    "    net_charge_prop = (composition.get('R',0) + composition.get('K',0)) - \\\n",
    "                      (composition.get('D',0) + composition.get('E',0))\n",
    "    bulky_hydrophobics_list = ['W', 'C', 'F', 'Y', 'I', 'V', 'L']\n",
    "    \n",
    "    return np.array([\n",
    "        hydro_norm_sum / valid_seq_len,\n",
    "        flex_norm_sum / valid_seq_len,\n",
    "        h_bond_potential_sum / valid_seq_len,\n",
    "        abs(net_charge_prop),\n",
    "        calculate_shannon_entropy(composition),\n",
    "        composition.get('P', 0),\n",
    "        sum(composition.get(aa, 0) for aa in bulky_hydrophobics_list)\n",
    "    ])\n",
    "\n",
    "def compute_features_for_dataset(sequence_list, window_size_param=None):\n",
    "    \"\"\"\n",
    "    Computes the new 7 features for a list of sequence strings.\n",
    "    If window_size_param is None, computes global features.\n",
    "    If window_size_param is an int, computes features for each window and averages them.\n",
    "    \"\"\"\n",
    "    all_feature_vectors = []\n",
    "    for seq_str in sequence_list:\n",
    "        if not seq_str: \n",
    "            all_feature_vectors.append(np.zeros(7))\n",
    "            continue\n",
    "        \n",
    "        canonical_sequence = \"\".join([aa for aa in seq_str if aa in canonical_set])\n",
    "        if not canonical_sequence: \n",
    "            all_feature_vectors.append(np.zeros(7))\n",
    "            continue\n",
    "\n",
    "        if window_size_param is None or len(canonical_sequence) < window_size_param:\n",
    "            all_feature_vectors.append(compute_new_seven_features(canonical_sequence))\n",
    "        else:\n",
    "            window_derived_feature_sets = [] \n",
    "            for i in range(len(canonical_sequence) - window_size_param + 1):\n",
    "                window_segment_str = canonical_sequence[i : i + window_size_param]\n",
    "                window_features = compute_new_seven_features(window_segment_str)\n",
    "                window_derived_feature_sets.append(window_features)\n",
    "            if not window_derived_feature_sets:\n",
    "                all_feature_vectors.append(compute_new_seven_features(canonical_sequence)) # Fallback\n",
    "            else:\n",
    "                all_feature_vectors.append(np.mean(np.vstack(window_derived_feature_sets), axis=0))\n",
    "    return all_feature_vectors\n",
    "\n",
    "# --- Prepare data for Global New Features Classifier ---\n",
    "new_feature_names = [\n",
    "    \"hydro_norm_avg\", \"flex_norm_avg\", \"h_bond_potential_avg\",\n",
    "    \"abs_net_charge_prop\", \"shannon_entropy\", \"freq_proline\", \"freq_bulky_hydrophobics\"\n",
    "]\n",
    "print(f\"\\nComputing NEW GLOBAL features (WINDOW_SIZE_GLOBAL_FEATURES = {WINDOW_SIZE_GLOBAL_FEATURES})...\")\n",
    "raw_sequences_list = [item['sequence'] for item in all_sequences_data]\n",
    "labels_list = [item['label'] for item in all_sequences_data]\n",
    "\n",
    "global_features_calculated = compute_features_for_dataset(raw_sequences_list, window_size_param=WINDOW_SIZE_GLOBAL_FEATURES)\n",
    "\n",
    "df_global_features = pd.DataFrame(global_features_calculated, columns=new_feature_names)\n",
    "df_global_features[\"label\"] = labels_list\n",
    "print(\"NEW GLOBAL feature computation complete.\")\n",
    "\n",
    "if df_global_features.empty or df_global_features['label'].nunique() < 2:\n",
    "    print(\"Error: Not enough data for global features. Exiting.\"); exit()\n",
    "    \n",
    "X_global_train, X_global_test, y_global_train, y_global_test, train_indices_global, test_indices_global = train_test_split(\n",
    "    df_global_features.drop(columns=[\"label\"]),\n",
    "    df_global_features[\"label\"],\n",
    "    np.arange(len(raw_sequences_list)), \n",
    "    test_size=0.2, random_state=42, stratify=df_global_features[\"label\"] \n",
    ")\n",
    "\n",
    "df_train_global_features = X_global_train.copy()\n",
    "df_train_global_features[\"label\"] = y_global_train\n",
    "\n",
    "# Raw sequences for the test set (will be used by the sliding window classifier)\n",
    "test_raw_sequences_for_sliding_window = [raw_sequences_list[i] for i in test_indices_global]\n",
    "y_test_for_sliding_window = y_global_test # True labels for the test set\n",
    "\n",
    "print(f\"\\nGlobal Features Training set size: {len(df_train_global_features)}\")\n",
    "print(f\"Global Features Testing set size: {len(X_global_test)}\")\n",
    "\n",
    "# --- Calculate Midpoints from Global New Features Training Data ---\n",
    "if df_train_global_features[\"label\"].nunique() < 2:\n",
    "    print(\"\\nError: Global features training set lacks class diversity for midpoints.\"); exit()\n",
    "else:\n",
    "    train_means_global = df_train_global_features.groupby(\"label\").mean().rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "    if \"PDB\" not in train_means_global.index or \"DisProt\" not in train_means_global.index:\n",
    "        print(\"\\nError: Could not find means for both PDB and DisProt in global training data.\"); exit()\n",
    "    else:\n",
    "        midpoints_global_new_features = {col: (train_means_global.loc[\"PDB\", col] + train_means_global.loc[\"DisProt\", col]) / 2\n",
    "                                         for col in X_global_train.columns}\n",
    "        print(\"\\nGlobal Feature Means (DisProt vs. PDB) from NEW GLOBAL FEATURES TRAINING DATA:\\n\")\n",
    "        print(train_means_global, \"\\n\")\n",
    "        print(\"Chosen Midpoint Thresholds (from NEW GLOBAL FEATURES TRAINING DATA):\\n\")\n",
    "        for feat, t in midpoints_global_new_features.items(): print(f\"  {feat:18s} = {t:.3f}\")\n",
    "        print()\n",
    "\n",
    "# --- Helper to count conditions met for the NEW 7 features ---\n",
    "def count_conditions_for_new_feature_vector(new_feature_vector_values, midpoints_dict, train_means_for_direction):\n",
    "    row = pd.Series(new_feature_vector_values, index=new_feature_names)\n",
    "    conditions_met_count = 0\n",
    "    \n",
    "    # hydro_norm_avg: PDB typically higher\n",
    "    if row[\"hydro_norm_avg\"] >= midpoints_dict.get(\"hydro_norm_avg\", 0.0): conditions_met_count +=1\n",
    "    # flex_norm_avg: PDB typically lower\n",
    "    if row[\"flex_norm_avg\"] <= midpoints_dict.get(\"flex_norm_avg\", float('inf')): conditions_met_count +=1\n",
    "    # h_bond_potential_avg: PDB typically lower\n",
    "    if row[\"h_bond_potential_avg\"] <= midpoints_dict.get(\"h_bond_potential_avg\", float('inf')): conditions_met_count +=1\n",
    "    # abs_net_charge_prop: PDB typically lower\n",
    "    if row[\"abs_net_charge_prop\"] <= midpoints_dict.get(\"abs_net_charge_prop\", float('inf')): conditions_met_count +=1\n",
    "    # shannon_entropy: PDB typically higher (inspect means to confirm this assumption)\n",
    "    if train_means_for_direction.loc[\"PDB\", \"shannon_entropy\"] > train_means_for_direction.loc[\"DisProt\", \"shannon_entropy\"]:\n",
    "        if row[\"shannon_entropy\"] >= midpoints_dict.get(\"shannon_entropy\", 0.0): conditions_met_count +=1\n",
    "    else: # PDB shannon_entropy is lower or equal\n",
    "        if row[\"shannon_entropy\"] <= midpoints_dict.get(\"shannon_entropy\", float('inf')): conditions_met_count +=1\n",
    "    # freq_proline: PDB typically lower\n",
    "    if row[\"freq_proline\"] <= midpoints_dict.get(\"freq_proline\", float('inf')): conditions_met_count +=1\n",
    "    # freq_bulky_hydrophobics: PDB typically higher\n",
    "    if row[\"freq_bulky_hydrophobics\"] >= midpoints_dict.get(\"freq_bulky_hydrophobics\", 0.0): conditions_met_count +=1\n",
    "    \n",
    "    return conditions_met_count\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# --- 1. New Global Features Threshold-Based Classifier Evaluation ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "print(\"\\n\\n--- Evaluating New Global Features Threshold-Based Classifier ---\")\n",
    "df_test_global_features_eval = X_global_test.copy()\n",
    "df_test_global_features_eval[\"label\"] = y_global_test\n",
    "\n",
    "if df_test_global_features_eval.empty:\n",
    "    print(\"Global features test set is empty. Skipping evaluation.\")\n",
    "else:\n",
    "    df_test_global_features_eval[\"conditions_met\"] = df_test_global_features_eval.apply(\n",
    "        lambda r: count_conditions_for_new_feature_vector(r[new_feature_names].values, midpoints_global_new_features, train_means_global), axis=1\n",
    "    )\n",
    "    \n",
    "    dist_test_global = df_test_global_features_eval.groupby(\"label\")[\"conditions_met\"].value_counts().unstack(fill_value=0).rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "    print(\"Distribution of ‘conditions_met’ (NEW GLOBAL features) by Label (ON TEST SET):\\n\")\n",
    "    print(dist_test_global, \"\\n\")\n",
    "\n",
    "    results_global = []\n",
    "    for k_thresh in range(1, 8):\n",
    "        preds_test_global = (df_test_global_features_eval[\"conditions_met\"] >= k_thresh).astype(int)\n",
    "        tp = ((preds_test_global == 1) & (y_global_test == 1)).sum()\n",
    "        fn = ((preds_test_global == 0) & (y_global_test == 1)).sum()\n",
    "        tn = ((preds_test_global == 0) & (y_global_test == 0)).sum()\n",
    "        fp = ((preds_test_global == 1) & (y_global_test == 0)).sum()\n",
    "        acc = (tp + tn) / len(y_global_test) if len(y_global_test) > 0 else 0\n",
    "        prec_pdb = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec_pdb = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_pdb = 2*(prec_pdb*rec_pdb)/(prec_pdb+rec_pdb) if (prec_pdb+rec_pdb)>0 else 0\n",
    "        results_global.append({\n",
    "            \"k\": k_thresh, \"TP\": tp, \"FN\": fn, \"TN\": tn, \"FP\": fp, \"Accuracy\": f\"{acc:.2%}\",\n",
    "            \"Precision (PDB)\": f\"{prec_pdb:.2%}\", \"Recall (PDB)\": f\"{rec_pdb:.2%}\", \"F1-score (PDB)\": f\"{f1_pdb:.2%}\"\n",
    "        })\n",
    "    df_results_global = pd.DataFrame(results_global)\n",
    "    print(\"Performance of New Global Features Classifier on TEST SET (varying k):\\n\")\n",
    "    print(df_results_global.to_string(index=False))\n",
    "\n",
    "    if not df_results_global.empty:\n",
    "        try:\n",
    "            df_results_global['F1_PDB_float'] = df_results_global['F1-score (PDB)'].str.rstrip('%').astype('float') / 100.0\n",
    "            best_k_row_global = df_results_global.loc[df_results_global['F1_PDB_float'].idxmax()]\n",
    "            best_k_global = int(best_k_row_global[\"k\"])\n",
    "            print(f\"\\n--- Detailed New Global Features Classification Report for best k = {best_k_global} (based on F1 PDB) ---\")\n",
    "            best_preds_global = (df_test_global_features_eval[\"conditions_met\"] >= best_k_global).astype(int)\n",
    "            print(classification_report(y_global_test, best_preds_global, target_names=[\"DisProt (0)\", \"PDB (1)\"], zero_division=0))\n",
    "            cm_global = confusion_matrix(y_global_test, best_preds_global)\n",
    "            print(\"Confusion Matrix for best k (New Global Features):\\n\", pd.DataFrame(cm_global, index=[\"Actual DisProt\",\"Actual PDB\"], columns=[\"Pred DisProt\",\"Pred PDB\"]))\n",
    "        except Exception as e: print(f\"Error in detailed report for global features: {e}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# --- 2. Sliding Window (Larger) Classifier with Failure Cancellation ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "print(\"\\n\\n--- Testing Sliding Window (Larger) Classifier with Failure Cancellation ---\")\n",
    "\n",
    "def classify_protein_sliding_window_cancellation_new_features(\n",
    "    sequence_str, window_size, slide_step,\n",
    "    midpoints_for_eval, window_k_pass_thresh, \n",
    "    max_allowed_total_failures, train_means_for_direction_check): # Added train_means\n",
    "\n",
    "    if not sequence_str: return 1 \n",
    "    canonical_sequence = \"\".join([aa for aa in sequence_str if aa in canonical_set])\n",
    "    if not canonical_sequence or len(canonical_sequence) < window_size: return 1 \n",
    "\n",
    "    current_consecutive_failures_streak = 0\n",
    "    num_windows_processed = 0\n",
    "\n",
    "    for i in range(0, len(canonical_sequence) - window_size + 1, slide_step):\n",
    "        window_str = canonical_sequence[i : i + window_size]\n",
    "        num_windows_processed += 1\n",
    "        \n",
    "        seven_new_features_for_current_window = compute_new_seven_features(window_str)\n",
    "        num_conditions_this_window_met = count_conditions_for_new_feature_vector(\n",
    "            seven_new_features_for_current_window, \n",
    "            midpoints_for_eval,\n",
    "            train_means_for_direction_check # Pass train_means here\n",
    "        )\n",
    "        \n",
    "        window_passes = (num_conditions_this_window_met >= window_k_pass_thresh)\n",
    "        \n",
    "        if window_passes: current_consecutive_failures_streak = 0 \n",
    "        else: current_consecutive_failures_streak += 1\n",
    "            \n",
    "    if num_windows_processed == 0: return 0 \n",
    "    total_unforgiven_failures = current_consecutive_failures_streak\n",
    "    \n",
    "    return 1 if total_unforgiven_failures <= max_allowed_total_failures else 0\n",
    "\n",
    "print(f\"\\nApplying Sliding Window (Larger) Classifier (NEW features) with Failure Cancellation: window_size={SLIDING_WINDOW_SIZE}, slide_step={SLIDING_WINDOW_SLIDE_STEP}, window_k_pass_thresh={SLIDING_WINDOW_PASS_K}, max_total_unforgiven_failures={MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING}...\")\n",
    "predictions_sliding_window_test = []\n",
    "if not test_raw_sequences_for_sliding_window:\n",
    "    print(\"No raw sequences in test set for sliding window classifier.\")\n",
    "else:\n",
    "    for raw_seq in test_raw_sequences_for_sliding_window:\n",
    "        pred = classify_protein_sliding_window_cancellation_new_features(\n",
    "            raw_seq, SLIDING_WINDOW_SIZE, SLIDING_WINDOW_SLIDE_STEP,\n",
    "            midpoints_global_new_features, # Use midpoints from global new features training\n",
    "            SLIDING_WINDOW_PASS_K,\n",
    "            MAX_UNFORGIVEN_FAILED_WINDOWS_SLIDING,\n",
    "            train_means_global # Pass train_means for direction check\n",
    "        )\n",
    "        predictions_sliding_window_test.append(pred)\n",
    "    print(\"\\nSliding Window (Larger) with Failure Cancellation classification complete.\")\n",
    "\n",
    "    if predictions_sliding_window_test:\n",
    "        print(\"\\nPerformance of Sliding Window (Larger) Classifier with Failure Cancellation (ON TEST SET):\\n\")\n",
    "        print(classification_report(y_test_for_sliding_window, predictions_sliding_window_test, target_names=[\"DisProt (0)\", \"PDB (1)\"], zero_division=0))\n",
    "        cm_sliding = confusion_matrix(y_test_for_sliding_window, predictions_sliding_window_test)\n",
    "        print(\"Confusion Matrix:\\n\", pd.DataFrame(cm_sliding, index=[\"Actual DisProt\",\"Actual PDB\"], columns=[\"Pred DisProt\",\"Pred PDB\"]))\n",
    "        acc_sliding = (cm_sliding[0,0] + cm_sliding[1,1]) / np.sum(cm_sliding) if np.sum(cm_sliding) > 0 else 0\n",
    "        print(f\"Accuracy: {acc_sliding:.2%}\")\n",
    "    else:\n",
    "        print(\"No predictions made by Sliding Window (Larger) classifier.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
