{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b534985e-b516-41af-b343-14da11c248bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Extracted 15000 chains → 'pdb_chains.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 1.) Download the PDB chain sequences (FASTA format from RCSB) via the HTTPS mirror,\n",
    "#      then keep only the first 15 000 entries.\n",
    "\n",
    "import requests\n",
    "\n",
    "# Use the “files.wwpdb.org” HTTPS mirror instead of FTP\n",
    "pdb_url = \"https://files.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt\"\n",
    "try:\n",
    "    resp = requests.get(pdb_url, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text.strip()\n",
    "    if not text.startswith(\">\"):\n",
    "        raise RuntimeError(\"Downloaded content does not look like FASTA.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to download PDB chain sequences: {e}\")\n",
    "\n",
    "# Write the complete dump to a temporary file\n",
    "with open(\"pdb_chains.fasta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "# ─── Split the full FASTA into individual (header, sequence) tuples ─────────────\n",
    "def split_fasta(filepath):\n",
    "    sequences = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        header = None\n",
    "        seq_lines = []\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None:\n",
    "                    sequences.append((header, \"\".join(seq_lines)))\n",
    "                header = line\n",
    "                seq_lines = []\n",
    "            else:\n",
    "                seq_lines.append(line)\n",
    "        # Add the final sequence\n",
    "        if header is not None:\n",
    "            sequences.append((header, \"\".join(seq_lines)))\n",
    "    return sequences\n",
    "\n",
    "all_chains = split_fasta(\"pdb_chains.fasta\")\n",
    "\n",
    "# ─── Keep exactly the first 15 000 chains ─────────────────────────────────────────\n",
    "subset = all_chains[:15000]\n",
    "\n",
    "# ─── Write those 15 000 chains back to “pdb_chains.fasta” ───────────────────────\n",
    "with open(\"pdb_chains.fasta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for header, seq in subset:\n",
    "        f.write(f\"{header}\\n\")\n",
    "        f.write(f\"{seq}\\n\")\n",
    "\n",
    "print(f\"✔ Extracted {len(subset)} chains → 'pdb_chains.fasta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e30de29-e8f5-4fe7-abfd-58594dc600e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Successfully fetched 100 DisProt sequences in FASTA format → 'disprot_1000.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 2.) Use DisProt’s search endpoint with format=fasta\n",
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://disprot.org/api/search?format=fasta&limit=10000\"\n",
    "try:\n",
    "    resp = requests.get(url, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to GET DisProt FASTA via API: {e}\")\n",
    "\n",
    "text = resp.text.strip()\n",
    "\n",
    "# 2.2) Quick sanity check: FASTA must start with '>', not '<'\n",
    "if not text.startswith(\">\"):\n",
    "    raise RuntimeError(\n",
    "        \"Downloaded content does not look like FASTA. \"\n",
    "        \"If it begins with '<', you're still hitting an HTML page instead of raw FASTA.\"\n",
    "    )\n",
    "\n",
    "# 2.3) Write the 100 DisProt entries to a file\n",
    "with open(\"disprot_13000.fasta\", \"w\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "print(\"✔ Successfully fetched 100 DisProt sequences in FASTA format → 'disprot_1000.fasta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b9b660b-dd1e-48ac-a9d2-8dc830cae3d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Fetched 25000 DisProt sequences → 'disprot_13000.fasta'\n"
     ]
    }
   ],
   "source": [
    "# 2.1) Collect more data\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# ─── PARAMETERS ─────────────────────────────────────────────────────────────\n",
    "TOTAL_DESIRED = 25_000   # how many DisProt sequences we want total\n",
    "PER_PAGE      = 100      # DisProt’s hard cap per request\n",
    "OUTPUT_FILE   = \"disprot_13000.fasta\"\n",
    "\n",
    "accum_seqs = []\n",
    "offset     = 0\n",
    "\n",
    "while len(accum_seqs) < TOTAL_DESIRED:\n",
    "    url = f\"https://disprot.org/api/search?format=fasta&limit={PER_PAGE}&offset={offset}\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to GET DisProt FASTA (offset={offset}): {e}\")\n",
    "\n",
    "    block = resp.text.strip()\n",
    "    if not block.startswith(\">\"):\n",
    "        raise RuntimeError(\n",
    "            \"Downloaded content does not look like FASTA. \"\n",
    "            \"If it begins with '<', you're still hitting an HTML page.\"\n",
    "        )\n",
    "\n",
    "    # Parse out this page’s FASTA sequences (collecting only the raw sequences, not full headers):\n",
    "    raw_lines = block.splitlines()\n",
    "    header = None\n",
    "    seq_buf = \"\"\n",
    "    this_page_seqs = []\n",
    "    for line in raw_lines:\n",
    "        if line.startswith(\">\"):\n",
    "            if header is not None and seq_buf:\n",
    "                this_page_seqs.append(seq_buf)\n",
    "            header = line\n",
    "            seq_buf = \"\"\n",
    "        else:\n",
    "            seq_buf += line.strip()\n",
    "    if header is not None and seq_buf:\n",
    "        this_page_seqs.append(seq_buf)\n",
    "\n",
    "    if not this_page_seqs:\n",
    "        # No more sequences returned → break out early\n",
    "        break\n",
    "\n",
    "    accum_seqs.extend(this_page_seqs)\n",
    "    offset += PER_PAGE\n",
    "\n",
    "    # Sleep briefly (so we don’t hammer the server)\n",
    "    time.sleep(0.4)\n",
    "\n",
    "# Trim in case we overshot\n",
    "accum_seqs = accum_seqs[:TOTAL_DESIRED]\n",
    "\n",
    "# Write out ~25k sequences in FASTA format (with minimal headers)\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    for i, seq in enumerate(accum_seqs):\n",
    "        f.write(f\">disprot_sequence_{i+1}\\n\")\n",
    "        f.write(seq + \"\\n\")\n",
    "\n",
    "print(f\"✔ Fetched {len(accum_seqs)} DisProt sequences → '{OUTPUT_FILE}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4177fdf8-80da-43a8-8fa1-2633860ed0cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">disprot_sequence_1\n",
      "EHVIEMDVTSENGQRALKEQSSKAKIVKNRWGRNVVQISNT\n",
      ">disprot_sequence_2\n",
      "VYRNSRAQGGG\n",
      ">disprot_sequence_3\n"
     ]
    }
   ],
   "source": [
    "# 2.2) Verify Downloaded Sequences\n",
    "with open(\"disprot_13000.fasta\") as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline().rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45e26bd9-9435-412f-9291-a62ae4e8477c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Feature Means (DisProt vs. PDB):\n",
      "\n",
      "         hydro_norm    charge    h_dh_a  norm_flex  pol_norm  arom_plus_helix  \\\n",
      "label                                                                           \n",
      "DisProt    0.401170 -0.023269  1.255558   0.837162  0.511919         0.718951   \n",
      "PDB        0.475469 -0.008521  1.071916   0.806189  0.436776         0.734963   \n",
      "\n",
      "         asa_norm  \n",
      "label              \n",
      "DisProt  0.519567  \n",
      "PDB      0.444721   \n",
      "\n",
      "Chosen Midpoint Thresholds:\n",
      "\n",
      "  hydro_norm         = 0.438\n",
      "  charge             = -0.016\n",
      "  h_dh_a             = 1.164\n",
      "  norm_flex          = 0.822\n",
      "  pol_norm           = 0.474\n",
      "  arom_plus_helix    = 0.727\n",
      "  asa_norm           = 0.482\n",
      "\n",
      "Distribution of ‘conditions_met’ by Label:\n",
      "\n",
      "conditions_met     0     1     2     3     4     5     6     7\n",
      "label                                                         \n",
      "DisProt         3116  3265  1762  1342  1204   980   922   196\n",
      "PDB              156   492   812   925  1525  2832  5579  2679 \n",
      "\n",
      "Performance as we vary k = minimum # of satisfied conditions:\n",
      "\n",
      " k (min # of features)    TP    FN    TN   FP Accuracy\n",
      "                     1 14844   156  3116 9671   64.63%\n",
      "                     2 14352   648  6381 6406   74.61%\n",
      "                     3 13540  1460  8143 4644   78.03%\n",
      "                     4 12615  2385  9485 3302   79.53%\n",
      "                     5 11090  3910 10689 2098   78.38%\n",
      "                     6  8258  6742 11669 1118   71.71%\n",
      "                     7  2679 12321 12591  196   54.95%\n"
     ]
    }
   ],
   "source": [
    "# 3.1 ) Seven‐Feature Threshold‐Based Fold/Disorder Classifier\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ─── (A) Build aa_properties ─────────────────────────────\n",
    "kd_hydro = {\n",
    "    'A':  1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C':  2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I':  4.5,\n",
    "    'L':  3.8, 'K': -3.9, 'M':  1.9, 'F':  2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V':  4.2\n",
    "}\n",
    "charge = {\n",
    "    'A':  0, 'R':  1, 'N':  0, 'D': -1, 'C':  0,\n",
    "    'Q':  0, 'E': -1, 'G':  0, 'H':  0, 'I':  0,\n",
    "    'L':  0, 'K':  1, 'M':  0, 'F':  0, 'P':  0,\n",
    "    'S':  0, 'T':  0, 'W':  0, 'Y':  0, 'V':  0\n",
    "}\n",
    "h_donors = {'A':0,'R':2,'N':2,'D':0,'C':0,'Q':2,'E':0,'G':0,'H':1,'I':0,\n",
    "            'L':0,'K':1,'M':0,'F':0,'P':0,'S':1,'T':1,'W':1,'Y':1,'V':0}\n",
    "h_acceptors = {'A':0,'R':0,'N':2,'D':2,'C':1,'Q':2,'E':2,'G':0,'H':1,'I':0,\n",
    "               'L':0,'K':0,'M':0,'F':0,'P':0,'S':1,'T':1,'W':0,'Y':1,'V':0}\n",
    "flexibility = {\n",
    "    'A': 0.357, 'R': 0.529, 'N': 0.463, 'D': 0.511, 'C': 0.346,\n",
    "    'Q': 0.493, 'E': 0.497, 'G': 0.544, 'H': 0.323, 'I': 0.462,\n",
    "    'L': 0.365, 'K': 0.466, 'M': 0.295, 'F': 0.314, 'P': 0.509,\n",
    "    'S': 0.507, 'T': 0.444, 'W': 0.305, 'Y': 0.420, 'V': 0.386\n",
    "}\n",
    "sidechain_volume = {\n",
    "    'A':  88.6, 'R': 173.4, 'N': 114.1, 'D': 111.1, 'C': 108.5,\n",
    "    'Q': 143.8, 'E': 138.4, 'G':  60.1, 'H': 153.2, 'I': 166.7,\n",
    "    'L': 166.7, 'K': 168.6, 'M': 162.9, 'F': 189.9, 'P': 112.7,\n",
    "    'S':  89.0, 'T': 116.1, 'W': 227.8, 'Y': 193.6, 'V': 140.0\n",
    "}\n",
    "polarity = {\n",
    "    'A':  8.1, 'R': 10.5, 'N': 11.6, 'D': 13.0, 'C':  5.5,\n",
    "    'Q': 10.5, 'E': 12.3, 'G':  9.0, 'H': 10.4, 'I':  5.2,\n",
    "    'L':  4.9, 'K': 11.3, 'M':  5.7, 'F':  5.2, 'P':  8.0,\n",
    "    'S':  9.2, 'T':  8.6, 'W':  5.4, 'Y':  6.2, 'V':  5.9\n",
    "}\n",
    "choufa_helix = {\n",
    "    'A': 1.45, 'R': 0.79, 'N': 0.73, 'D': 1.01, 'C': 0.77,\n",
    "    'Q': 1.17, 'E': 1.51, 'G': 0.53, 'H': 1.00, 'I': 1.08,\n",
    "    'L': 1.34, 'K': 1.07, 'M': 1.20, 'F': 1.12, 'P': 0.59,\n",
    "    'S': 0.79, 'T': 0.82, 'W': 1.14, 'Y': 0.61, 'V': 1.06\n",
    "}\n",
    "choufa_sheet = {\n",
    "    'A': 0.97, 'R': 0.90, 'N': 0.65, 'D': 0.54, 'C': 1.30,\n",
    "    'Q': 1.23, 'E': 0.37, 'G': 0.75, 'H': 0.87, 'I': 1.60,\n",
    "    'L': 1.22, 'K': 0.74, 'M': 1.67, 'F': 1.28, 'P': 0.62,\n",
    "    'S': 0.72, 'T': 1.20, 'W': 1.19, 'Y': 1.29, 'V': 1.70\n",
    "}\n",
    "rel_ASA = {\n",
    "    'A': 0.74, 'R': 1.48, 'N': 1.14, 'D': 1.23, 'C': 0.86,\n",
    "    'Q': 1.36, 'E': 1.26, 'G': 1.00, 'H': 0.91, 'I': 0.59,\n",
    "    'L': 0.61, 'K': 1.29, 'M': 0.64, 'F': 0.65, 'P': 0.71,\n",
    "    'S': 1.42, 'T': 1.20, 'W': 0.55, 'Y': 0.63, 'V': 0.54\n",
    "}\n",
    "beta_branched = {aa: (1 if aa in ('V','I','T') else 0) for aa in kd_hydro.keys()}\n",
    "\n",
    "# Build aa_properties dictionary (12 dimensions per residue)\n",
    "aa_properties = {}\n",
    "canonical_set = set(kd_hydro.keys())\n",
    "for aa in canonical_set:\n",
    "    hydro_norm  = (kd_hydro[aa] + 4.5) / 9.0\n",
    "    volume_norm = sidechain_volume[aa] / 227.8\n",
    "    pol_norm    = (polarity[aa] - 4.9) / (13.0 - 4.9)\n",
    "    helix_norm  = choufa_helix[aa] / 1.51\n",
    "    sheet_norm  = choufa_sheet[aa] / 1.70\n",
    "    asa_norm    = (rel_ASA[aa] - 0.54) / (1.48 - 0.54)\n",
    "    aromatic    = 1 if aa in ('F','Y','W') else 0\n",
    "\n",
    "    aa_properties[aa] = [\n",
    "        hydro_norm,          # [0]\n",
    "        charge[aa],          # [1]\n",
    "        h_donors[aa],        # [2]\n",
    "        h_acceptors[aa],     # [3]\n",
    "        flexibility[aa],     # [4]\n",
    "        volume_norm,         # [5]\n",
    "        pol_norm,            # [6]\n",
    "        aromatic,            # [7]\n",
    "        helix_norm,          # [8]\n",
    "        sheet_norm,          # [9]\n",
    "        asa_norm,            # [10]\n",
    "        beta_branched[aa]    # [11]\n",
    "    ]\n",
    "\n",
    "# ─── (B) Load FASTA sequences ─────────────────────────────────────────────────\n",
    "def load_fasta(filepath, filter_non_canonical=False):\n",
    "    seqs = []\n",
    "    with open(filepath) as f:\n",
    "        header = None\n",
    "        seq = \"\"\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None and seq:\n",
    "                    if (not filter_non_canonical) or (set(seq) <= canonical_set):\n",
    "                        seqs.append(seq)\n",
    "                header = line\n",
    "                seq = \"\"\n",
    "            else:\n",
    "                seq += line\n",
    "        if header is not None and seq:\n",
    "            if (not filter_non_canonical) or (set(seq) <= canonical_set):\n",
    "                seqs.append(seq)\n",
    "    return seqs\n",
    "\n",
    "pdb_seqs    = load_fasta(\"pdb_chains.fasta\",   filter_non_canonical=False)   # 70 PDB chains\n",
    "disprot_seqs = load_fasta(\"disprot_13000.fasta\", filter_non_canonical=False)  # 100 DisProt\n",
    "\n",
    "# ─── (C) Compute each chain’s 7 global features ────────────────────────────────\n",
    "def compute_global_features(sequence):\n",
    "    props = []\n",
    "    for aa in sequence:\n",
    "        if aa in aa_properties:\n",
    "            v = aa_properties[aa]\n",
    "            props.append([\n",
    "                v[0],               # hydrophobicity_norm\n",
    "                v[1],               # charge\n",
    "                v[2] + v[3],        # h_dh_a\n",
    "                v[4] / 0.544,       # norm_flex (raw_flex/0.544)\n",
    "                v[6],               # pol_norm\n",
    "                v[7] + v[8],        # arom_plus_helix\n",
    "                v[10]               # asa_norm\n",
    "            ])\n",
    "    if not props:\n",
    "        return np.zeros(7)\n",
    "    return np.mean(np.vstack(props), axis=0)\n",
    "\n",
    "all_features = []\n",
    "all_labels   = []\n",
    "\n",
    "for seq in pdb_seqs:\n",
    "    all_features.append(compute_global_features(seq))\n",
    "    all_labels.append(1)   # 1 = folded (PDB)\n",
    "for seq in disprot_seqs:\n",
    "    all_features.append(compute_global_features(seq))\n",
    "    all_labels.append(0)   # 0 = disordered (DisProt)\n",
    "\n",
    "df_feat = pd.DataFrame(\n",
    "    all_features,\n",
    "    columns=[\n",
    "        \"hydro_norm\",\n",
    "        \"charge\",\n",
    "        \"h_dh_a\",\n",
    "        \"norm_flex\",\n",
    "        \"pol_norm\",\n",
    "        \"arom_plus_helix\",\n",
    "        \"asa_norm\"\n",
    "    ]\n",
    ")\n",
    "df_feat[\"label\"] = all_labels\n",
    "\n",
    "# ─── (D) Compute midpoint thresholds (mean of PDB vs. mean of DisProt) ───────\n",
    "means = df_feat.groupby(\"label\").mean().rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "midpoints = {col: (means.loc[\"PDB\", col] + means.loc[\"DisProt\", col]) / 2\n",
    "             for col in df_feat.columns[:-1]}\n",
    "\n",
    "print(\"Global Feature Means (DisProt vs. PDB):\\n\")\n",
    "print(means, \"\\n\")\n",
    "print(\"Chosen Midpoint Thresholds:\\n\")\n",
    "for feat, t in midpoints.items():\n",
    "    print(f\"  {feat:18s} = {t:.3f}\")\n",
    "print()\n",
    "\n",
    "# ─── (E) Count how many of the 7 conditions each chain satisfies ───────────────\n",
    "def count_conditions(row):\n",
    "    c1 = row[\"hydro_norm\"]          >= midpoints[\"hydro_norm\"]\n",
    "    c2 = abs(row[\"charge\"])         <= abs(midpoints[\"charge\"])\n",
    "    c3 = row[\"h_dh_a\"]              <= midpoints[\"h_dh_a\"]\n",
    "    c4 = row[\"norm_flex\"]           <= midpoints[\"norm_flex\"]\n",
    "    c5 = row[\"pol_norm\"]            <= midpoints[\"pol_norm\"]\n",
    "    c6 = row[\"arom_plus_helix\"]     >= midpoints[\"arom_plus_helix\"]\n",
    "    c7 = row[\"asa_norm\"]            <= midpoints[\"asa_norm\"]\n",
    "    return sum([c1, c2, c3, c4, c5, c6, c7])\n",
    "\n",
    "df_feat[\"conditions_met\"] = df_feat.apply(count_conditions, axis=1)\n",
    "\n",
    "# Show the distribution of “conditions_met” separately for PDB vs. DisProt\n",
    "dist = df_feat.groupby(\"label\")[\"conditions_met\"] \\\n",
    "              .value_counts() \\\n",
    "              .unstack(fill_value=0) \\\n",
    "              .rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"Distribution of ‘conditions_met’ by Label:\\n\")\n",
    "print(dist, \"\\n\")\n",
    "\n",
    "# ─── (F) For each k=1…7, classify “folded if conditions_met ≥ k” ─────────────\n",
    "results = []\n",
    "for k in range(1, 8):\n",
    "    preds = (df_feat[\"conditions_met\"] >= k).astype(int)\n",
    "    tp = ((preds == 1) & (df_feat[\"label\"] == 1)).sum()\n",
    "    fn = ((preds == 0) & (df_feat[\"label\"] == 1)).sum()\n",
    "    tn = ((preds == 0) & (df_feat[\"label\"] == 0)).sum()\n",
    "    fp = ((preds == 1) & (df_feat[\"label\"] == 0)).sum()\n",
    "    acc = (tp + tn) / len(df_feat)\n",
    "    results.append({\n",
    "        \"k (min # of features)\": k,\n",
    "        \"TP\": tp,\n",
    "        \"FN\": fn,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"Accuracy\": f\"{acc:.2%}\"\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"Performance as we vary k = minimum # of satisfied conditions:\\n\")\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a0e701c-429d-44dd-bb1b-1e7bf3939c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.53576445  1.399552    3.16928867 -3.2788265   0.62568944  2.48338204\n",
      "  0.4729385   1.03119605]\n",
      "[-7.82062235]\n"
     ]
    }
   ],
   "source": [
    "# 3.2. Logistic Regression–Derived Seven‐Feature Classifier\n",
    " \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# (1) Split the same feature matrix and label vector into train/test\n",
    "X = df_feat.drop(columns=[\"label\"])\n",
    "y = df_feat[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# (2) Fit the logistic model (with class_weight='balanced'):\n",
    "clf = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs',\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# (3) After fitting, these attributes hold exactly the numbers we used:\n",
    "print(clf.coef_.flatten())   # → [ 9.149,  3.051,  2.034, -7.553, -6.521,  8.728, -7.629 ]\n",
    "print(clf.intercept_)        # → [0.131]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "971e310d-127c-4f23-bd62-e7ca5b94c106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Threshold = 0.005 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     DisProt       1.00      0.72      0.84     25000\n",
      "         PDB       0.01      0.63      0.01        71\n",
      "\n",
      "    accuracy                           0.72     25071\n",
      "   macro avg       0.50      0.68      0.42     25071\n",
      "weighted avg       1.00      0.72      0.83     25071\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt         18006      6994\n",
      "Actual PDB                26        45\n",
      "\n",
      "--- Threshold = 0.010 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     DisProt       1.00      0.76      0.87     25000\n",
      "         PDB       0.01      0.49      0.01        71\n",
      "\n",
      "    accuracy                           0.76     25071\n",
      "   macro avg       0.50      0.63      0.44     25071\n",
      "weighted avg       1.00      0.76      0.86     25071\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt         19101      5899\n",
      "Actual PDB                36        35\n",
      "\n",
      "--- Threshold = 0.020 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     DisProt       1.00      0.81      0.89     25000\n",
      "         PDB       0.00      0.34      0.01        71\n",
      "\n",
      "    accuracy                           0.81     25071\n",
      "   macro avg       0.50      0.57      0.45     25071\n",
      "weighted avg       0.99      0.81      0.89     25071\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt         20207      4793\n",
      "Actual PDB                47        24\n",
      "\n",
      "--- Threshold = 0.050 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     DisProt       1.00      0.85      0.92     25000\n",
      "         PDB       0.00      0.04      0.00        71\n",
      "\n",
      "    accuracy                           0.85     25071\n",
      "   macro avg       0.50      0.45      0.46     25071\n",
      "weighted avg       0.99      0.85      0.92     25071\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt         21358      3642\n",
      "Actual PDB                68         3\n",
      "\n",
      "--- Threshold = 0.100 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     DisProt       1.00      0.89      0.94     25000\n",
      "         PDB       0.00      0.01      0.00        71\n",
      "\n",
      "    accuracy                           0.89     25071\n",
      "   macro avg       0.50      0.45      0.47     25071\n",
      "weighted avg       0.99      0.89      0.94     25071\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred DisProt  Pred PDB\n",
      "Actual DisProt         22199      2801\n",
      "Actual PDB                70         1\n",
      "\n",
      "Probability Statistics:\n",
      "  DisProt mean prob: 0.0617\n",
      "  PDB     mean prob: 0.0177\n",
      "  Overall range: 0.0000 → 1.0000\n"
     ]
    }
   ],
   "source": [
    "# ─── 3.3 ) Rule-Based Seven-Feature Classifier (Using Correct Standardization) ───\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# (A) We assume `df_feat` already exists and has columns:\n",
    "#     [\"hydro_norm\",\"charge\",\"h_dh_a\",\"norm_flex\",\"pol_norm\",\"arom_plus_helix\",\"asa_norm\",\"label\"]\n",
    "\n",
    "feature_cols = [\n",
    "    \"hydro_norm\",\n",
    "    \"charge\",\n",
    "    \"h_dh_a\",\n",
    "    \"norm_flex\",\n",
    "    \"pol_norm\",\n",
    "    \"arom_plus_helix\",\n",
    "    \"asa_norm\"\n",
    "]\n",
    "\n",
    "# ─── (B) Compute means/stds of each feature (exactly what was done before training) ───\n",
    "# (In practice, you would have saved these when you fit the original LR. \n",
    "#  Here we re‐compute them on the same df_feat so that we standardize identically.)\n",
    "means = df_feat[feature_cols].mean().values   # shape = (7,)\n",
    "stds  = df_feat[feature_cols].std(ddof=0).values   # ddof=0 → population std, same as StandardScaler\n",
    "\n",
    "# ─── (C) Standardize the entire dataset (N × 7) ──────────────────────────────────────\n",
    "X_raw    = df_feat[feature_cols].values        # (N,7) raw features\n",
    "X_scaled = (X_raw - means) / stds              # (N,7) standardized exact\n",
    "\n",
    "# ─── (D) Plug in the “new” learned weights + intercept ─────────────────────────────\n",
    "# These came from your corrected LR training run on scaled data:\n",
    "weights   = np.array([\n",
    "    0.53576445,   # hydro_norm\n",
    "    1.399552  ,   # charge\n",
    "    3.16928867,   # h_dh_a\n",
    "   -3.2788265 ,   # norm_flex\n",
    "    0.62568944,   # pol_norm\n",
    "    2.48338204,   # arom_plus_helix\n",
    "    0.4729385    # asa_norm\n",
    "])\n",
    "intercept = -7.82062235\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# (E) Compute raw LR‐score and probability(p) = sigmoid(score)\n",
    "raw_scores = intercept + X_scaled.dot(weights)  # (N,)\n",
    "probs_pdb  = sigmoid(raw_scores)                # (N,) “probability of class=1 (PDB)”\n",
    "\n",
    "# ─── (F) Now choose a threshold << 0.5 (e.g. 0.02) to call “PDB” ─────────────────\n",
    "# We’ll scan a few candidate thresholds to see where some PDB are rescued:\n",
    "for thresh in [0.005, 0.01, 0.02, 0.05, 0.10]:\n",
    "    preds = (probs_pdb > thresh).astype(int)\n",
    "    print(f\"\\n--- Threshold = {thresh:.3f} ---\")\n",
    "    print(classification_report(df_feat[\"label\"], preds, target_names=[\"DisProt\",\"PDB\"]))\n",
    "    cm = confusion_matrix(df_feat[\"label\"], preds)\n",
    "    cm_df = pd.DataFrame(cm, index=[\"Actual DisProt\",\"Actual PDB\"], columns=[\"Pred DisProt\",\"Pred PDB\"])\n",
    "    print(\"Confusion Matrix:\\n\", cm_df)\n",
    "\n",
    "# ─── (G) (Optional) Print out basic probability summary to see the gap ───────────\n",
    "disprot_mask = (df_feat[\"label\"] == 0)\n",
    "pdb_mask     = (df_feat[\"label\"] == 1)\n",
    "print(\"\\nProbability Statistics:\")\n",
    "print(f\"  DisProt mean prob: {probs_pdb[disprot_mask].mean():.4f}\")\n",
    "print(f\"  PDB     mean prob: {probs_pdb[pdb_mask].mean():.4f}\")\n",
    "print(f\"  Overall range: {probs_pdb.min():.4f} → {probs_pdb.max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a8f8c86-95fc-457e-962a-4b3221f55727",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Threshold = 0.5, CORRECTED WEIGHTS):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     DisProt       1.00      1.00      1.00     25000\n",
      "         PDB       0.00      0.00      0.00        70\n",
      "\n",
      "    accuracy                           1.00     25070\n",
      "   macro avg       0.50      0.50      0.50     25070\n",
      "weighted avg       0.99      1.00      1.00     25070\n",
      "\n",
      "Confusion Matrix (Threshold = 0.5):\n",
      "\n",
      "                Pred DisProt  Pred PDB\n",
      "Actual DisProt         24961        39\n",
      "Actual PDB                70         0\n",
      "\n",
      "--- Threshold = 0.7 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     DisProt       1.00      1.00      1.00     25000\n",
      "         PDB       0.00      0.00      0.00        70\n",
      "\n",
      "    accuracy                           1.00     25070\n",
      "   macro avg       0.50      0.50      0.50     25070\n",
      "weighted avg       0.99      1.00      1.00     25070\n",
      "\n",
      "Confusion Matrix (Threshold = 0.7):\n",
      "\n",
      "                Pred DisProt  Pred PDB\n",
      "Actual DisProt         24992         8\n",
      "Actual PDB                70         0\n",
      "\n",
      "CORRECTED Logistic Weights:\n",
      "hydro_norm      → +1.164\n",
      "charge          → +1.857\n",
      "h_dh_a          → +3.499\n",
      "norm_flex       → -1.849\n",
      "pol_norm        → -0.910\n",
      "arom_plus_helix → +2.029\n",
      "asa_norm        → -0.315\n",
      "Intercept: -7.823\n",
      "\n",
      "Probability Statistics:\n",
      "DisProt proteins - mean prob of being 'folded': 0.0362\n",
      "PDB proteins - mean prob of being 'folded': 0.0238\n",
      "Overall range: 0.0003 to 0.9950\n"
     ]
    }
   ],
   "source": [
    "# 3.3a Rule‐Based Seven‐Feature Classifier (Using KNOWN Learned LR Weights)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ─── CORRECTED WEIGHTS FROM ACTUAL SKLEARN FIT ─────────────────────────────\n",
    "# From section 3.2, the actual learned weights were:\n",
    "# [ 1.16407641  1.85715342  3.49913782 -1.84923829 -0.91043986  2.02929407\n",
    "#  -0.31493117  0.9647182 ]\n",
    "# [-7.8232685]\n",
    "\n",
    "# Store the CORRECT weights and intercept\n",
    "weights = np.array([\n",
    "    1.164,    # weight for hydro_norm\n",
    "    1.857,    # weight for charge  \n",
    "    3.499,    # weight for h_dh_a\n",
    "   -1.849,    # weight for norm_flex\n",
    "   -0.910,    # weight for pol_norm\n",
    "    2.029,    # weight for arom_plus_helix\n",
    "   -0.315     # weight for asa_norm\n",
    "])\n",
    "intercept = -7.823  # CORRECTED from +0.131 to -7.823\n",
    "\n",
    "# Compute \"score\" and predicted probability for each protein in df_feat\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# Extract feature matrix (N × 7) - assuming df_feat exists from previous code\n",
    "X = df_feat[[\n",
    "    \"hydro_norm\",\n",
    "    \"charge\", \n",
    "    \"h_dh_a\",\n",
    "    \"norm_flex\",\n",
    "    \"pol_norm\",\n",
    "    \"arom_plus_helix\",\n",
    "    \"asa_norm\"\n",
    "]].values\n",
    "\n",
    "# Compute raw scores: intercept + X ⋅ weights\n",
    "raw_scores = intercept + X.dot(weights)\n",
    "\n",
    "# Compute predicted probabilities of \"PDB\" (folded)\n",
    "probs_pdb = sigmoid(raw_scores)\n",
    "\n",
    "# Choose threshold = 0.5 for \"PDB\" vs. \"DisProt\"\n",
    "preds_05 = (probs_pdb > 0.5).astype(int)\n",
    "\n",
    "# ─── Evaluate on the Entire Dataset ─────────────────────────────────────────\n",
    "true_labels = df_feat[\"label\"].values\n",
    "\n",
    "print(\"Classification Report (Threshold = 0.5, CORRECTED WEIGHTS):\\n\")\n",
    "print(classification_report(true_labels, preds_05, target_names=[\"DisProt\",\"PDB\"]))\n",
    "\n",
    "cm = confusion_matrix(true_labels, preds_05)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual DisProt\", \"Actual PDB\"],\n",
    "    columns=[\"Pred DisProt\", \"Pred PDB\"]\n",
    ")\n",
    "print(\"Confusion Matrix (Threshold = 0.5):\\n\")\n",
    "print(cm_df)\n",
    "\n",
    "# ─── Threshold = 0.7 ──────────────────────────────────────────────────────────\n",
    "preds_07 = (probs_pdb > 0.7).astype(int)\n",
    "print(\"\\n--- Threshold = 0.7 ---\")\n",
    "print(classification_report(true_labels, preds_07, target_names=[\"DisProt\",\"PDB\"]))\n",
    "cm_07 = confusion_matrix(true_labels, preds_07)\n",
    "cm_07_df = pd.DataFrame(\n",
    "    cm_07,\n",
    "    index=[\"Actual DisProt\", \"Actual PDB\"],\n",
    "    columns=[\"Pred DisProt\", \"Pred PDB\"]\n",
    ")\n",
    "print(\"Confusion Matrix (Threshold = 0.7):\\n\")\n",
    "print(cm_07_df)\n",
    "\n",
    "# ─── Display the CORRECTED Weights ────────────────────────────────────────────\n",
    "print(\"\\nCORRECTED Logistic Weights:\")\n",
    "feature_names = [\n",
    "    \"hydro_norm\", \"charge\", \"h_dh_a\",\n",
    "    \"norm_flex\", \"pol_norm\", \"arom_plus_helix\", \"asa_norm\"\n",
    "]\n",
    "for name, w in zip(feature_names, weights):\n",
    "    print(f\"{name:15s} → {w:+.3f}\")\n",
    "print(f\"Intercept: {intercept:+.3f}\")\n",
    "\n",
    "# ─── Show some probability distributions ──────────────────────────────────────\n",
    "print(f\"\\nProbability Statistics:\")\n",
    "print(f\"DisProt proteins - mean prob of being 'folded': {probs_pdb[true_labels==0].mean():.4f}\")\n",
    "print(f\"PDB proteins - mean prob of being 'folded': {probs_pdb[true_labels==1].mean():.4f}\")\n",
    "print(f\"Overall range: {probs_pdb.min():.4f} to {probs_pdb.max():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
