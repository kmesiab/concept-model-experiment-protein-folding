{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c34776-0b43-410a-8307-3a256dde0a8d",
   "metadata": {},
   "source": [
    "# 1.) Download the PDB chain sequences (FASTA format from RCSB)\n",
    "import requests\n",
    "\n",
    "pdb_url = \"https://ftp.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt\"\n",
    "try:\n",
    "    resp = requests.get(pdb_url, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text.strip()\n",
    "    if not text.startswith(\">\"):\n",
    "        raise RuntimeError(\"Downloaded content does not look like FASTA.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to download PDB chain sequences: {e}\")\n",
    "\n",
    "with open(\"pdb_chains.fasta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "print(\"✔ Successfully fetched PDB chain sequences → 'pdb_chains.fasta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30de29-e8f5-4fe7-abfd-58594dc600e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1) Use DisProt’s search endpoint with format=fasta\n",
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://disprot.org/api/search?format=fasta&limit=10000\"\n",
    "try:\n",
    "    resp = requests.get(url, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to GET DisProt FASTA via API: {e}\")\n",
    "\n",
    "text = resp.text.strip()\n",
    "\n",
    "# 2.2) Quick sanity check: FASTA must start with '>', not '<'\n",
    "if not text.startswith(\">\"):\n",
    "    raise RuntimeError(\n",
    "        \"Downloaded content does not look like FASTA. \"\n",
    "        \"If it begins with '<', you're still hitting an HTML page instead of raw FASTA.\"\n",
    "    )\n",
    "\n",
    "# 2.3) Write the 100 DisProt entries to a file\n",
    "with open(\"disprot_13000.fasta\", \"w\") as f:\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "print(\"✔ Successfully fetched 100 DisProt sequences in FASTA format → 'disprot_1000.fasta'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da41888-5f38-4b05-9c6d-1e0eca286228",
   "metadata": {},
   "source": [
    "# 2.1) Collect more data\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# ─── PARAMETERS ─────────────────────────────────────────────────────────────\n",
    "TOTAL_DESIRED = 13_000   # how many DisProt sequences we want total\n",
    "PER_PAGE      = 100      # DisProt’s hard cap per request\n",
    "OUTPUT_FILE   = \"disprot_13000.fasta\"\n",
    "\n",
    "accum_seqs = []\n",
    "offset     = 0\n",
    "\n",
    "while len(accum_seqs) < TOTAL_DESIRED:\n",
    "    url = f\"https://disprot.org/api/search?format=fasta&limit={PER_PAGE}&offset={offset}\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to GET DisProt FASTA (offset={offset}): {e}\")\n",
    "\n",
    "    block = resp.text.strip()\n",
    "    if not block.startswith(\">\"):\n",
    "        raise RuntimeError(\n",
    "            \"Downloaded content does not look like FASTA. \"\n",
    "            \"If it begins with '<', you're still hitting an HTML page.\"\n",
    "        )\n",
    "\n",
    "    # Parse out this page’s FASTA sequences (collecting only the raw sequences, not full headers):\n",
    "    raw_lines = block.splitlines()\n",
    "    header = None\n",
    "    seq_buf = \"\"\n",
    "    this_page_seqs = []\n",
    "    for line in raw_lines:\n",
    "        if line.startswith(\">\"):\n",
    "            if header is not None and seq_buf:\n",
    "                this_page_seqs.append(seq_buf)\n",
    "            header = line\n",
    "            seq_buf = \"\"\n",
    "        else:\n",
    "            seq_buf += line.strip()\n",
    "    if header is not None and seq_buf:\n",
    "        this_page_seqs.append(seq_buf)\n",
    "\n",
    "    if not this_page_seqs:\n",
    "        # No more sequences returned → break out early\n",
    "        break\n",
    "\n",
    "    accum_seqs.extend(this_page_seqs)\n",
    "    offset += PER_PAGE\n",
    "\n",
    "    # Sleep briefly (so we don’t hammer the server)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "# Trim in case we overshot\n",
    "accum_seqs = accum_seqs[:TOTAL_DESIRED]\n",
    "\n",
    "# Write out ~13k sequences in FASTA format (with minimal headers)\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    for i, seq in enumerate(accum_seqs):\n",
    "        f.write(f\">disprot_sequence_{i+1}\\n\")\n",
    "        f.write(seq + \"\\n\")\n",
    "\n",
    "print(f\"✔ Fetched {len(accum_seqs)} DisProt sequences → '{OUTPUT_FILE}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177fdf8-80da-43a8-8fa1-2633860ed0cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.2) Verify Downloaded Sequences\n",
    "with open(\"disprot_13000.fasta\") as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline().rstrip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7febb9-a9bc-4aae-b579-bce9fffa7a83",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression–Derived Seven‐Feature Classifier\n",
    " \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# (1) Split the same feature matrix and label vector into train/test\n",
    "X = df_feat.drop(columns=[\"label\"])\n",
    "y = df_feat[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# (2) Fit the logistic model (with class_weight='balanced'):\n",
    "clf = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs',\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# (3) After fitting, these attributes hold exactly the numbers we used:\n",
    "print(clf.coef_.flatten())   # → [ 9.149,  3.051,  2.034, -7.553, -6.521,  8.728, -7.629 ]\n",
    "print(clf.intercept_)        # → [0.131]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93dc95d1-eff6-4662-9685-946c59eee3c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 3.1 ) Seven‐Feature Threshold‐Based Fold/Disorder Classifier\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ─── (A) Build aa_properties exactly as in STEP X ─────────────────────────────\n",
    "kd_hydro = {\n",
    "    'A':  1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C':  2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I':  4.5,\n",
    "    'L':  3.8, 'K': -3.9, 'M':  1.9, 'F':  2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V':  4.2\n",
    "}\n",
    "charge = {\n",
    "    'A':  0, 'R':  1, 'N':  0, 'D': -1, 'C':  0,\n",
    "    'Q':  0, 'E': -1, 'G':  0, 'H':  0, 'I':  0,\n",
    "    'L':  0, 'K':  1, 'M':  0, 'F':  0, 'P':  0,\n",
    "    'S':  0, 'T':  0, 'W':  0, 'Y':  0, 'V':  0\n",
    "}\n",
    "h_donors = {'A':0,'R':2,'N':2,'D':0,'C':0,'Q':2,'E':0,'G':0,'H':1,'I':0,\n",
    "            'L':0,'K':1,'M':0,'F':0,'P':0,'S':1,'T':1,'W':1,'Y':1,'V':0}\n",
    "h_acceptors = {'A':0,'R':0,'N':2,'D':2,'C':1,'Q':2,'E':2,'G':0,'H':1,'I':0,\n",
    "               'L':0,'K':0,'M':0,'F':0,'P':0,'S':1,'T':1,'W':0,'Y':1,'V':0}\n",
    "flexibility = {\n",
    "    'A': 0.357, 'R': 0.529, 'N': 0.463, 'D': 0.511, 'C': 0.346,\n",
    "    'Q': 0.493, 'E': 0.497, 'G': 0.544, 'H': 0.323, 'I': 0.462,\n",
    "    'L': 0.365, 'K': 0.466, 'M': 0.295, 'F': 0.314, 'P': 0.509,\n",
    "    'S': 0.507, 'T': 0.444, 'W': 0.305, 'Y': 0.420, 'V': 0.386\n",
    "}\n",
    "sidechain_volume = {\n",
    "    'A':  88.6, 'R': 173.4, 'N': 114.1, 'D': 111.1, 'C': 108.5,\n",
    "    'Q': 143.8, 'E': 138.4, 'G':  60.1, 'H': 153.2, 'I': 166.7,\n",
    "    'L': 166.7, 'K': 168.6, 'M': 162.9, 'F': 189.9, 'P': 112.7,\n",
    "    'S':  89.0, 'T': 116.1, 'W': 227.8, 'Y': 193.6, 'V': 140.0\n",
    "}\n",
    "polarity = {\n",
    "    'A':  8.1, 'R': 10.5, 'N': 11.6, 'D': 13.0, 'C':  5.5,\n",
    "    'Q': 10.5, 'E': 12.3, 'G':  9.0, 'H': 10.4, 'I':  5.2,\n",
    "    'L':  4.9, 'K': 11.3, 'M':  5.7, 'F':  5.2, 'P':  8.0,\n",
    "    'S':  9.2, 'T':  8.6, 'W':  5.4, 'Y':  6.2, 'V':  5.9\n",
    "}\n",
    "choufa_helix = {\n",
    "    'A': 1.45, 'R': 0.79, 'N': 0.73, 'D': 1.01, 'C': 0.77,\n",
    "    'Q': 1.17, 'E': 1.51, 'G': 0.53, 'H': 1.00, 'I': 1.08,\n",
    "    'L': 1.34, 'K': 1.07, 'M': 1.20, 'F': 1.12, 'P': 0.59,\n",
    "    'S': 0.79, 'T': 0.82, 'W': 1.14, 'Y': 0.61, 'V': 1.06\n",
    "}\n",
    "choufa_sheet = {\n",
    "    'A': 0.97, 'R': 0.90, 'N': 0.65, 'D': 0.54, 'C': 1.30,\n",
    "    'Q': 1.23, 'E': 0.37, 'G': 0.75, 'H': 0.87, 'I': 1.60,\n",
    "    'L': 1.22, 'K': 0.74, 'M': 1.67, 'F': 1.28, 'P': 0.62,\n",
    "    'S': 0.72, 'T': 1.20, 'W': 1.19, 'Y': 1.29, 'V': 1.70\n",
    "}\n",
    "rel_ASA = {\n",
    "    'A': 0.74, 'R': 1.48, 'N': 1.14, 'D': 1.23, 'C': 0.86,\n",
    "    'Q': 1.36, 'E': 1.26, 'G': 1.00, 'H': 0.91, 'I': 0.59,\n",
    "    'L': 0.61, 'K': 1.29, 'M': 0.64, 'F': 0.65, 'P': 0.71,\n",
    "    'S': 1.42, 'T': 1.20, 'W': 0.55, 'Y': 0.63, 'V': 0.54\n",
    "}\n",
    "beta_branched = {aa: (1 if aa in ('V','I','T') else 0) for aa in kd_hydro.keys()}\n",
    "\n",
    "# Build aa_properties dictionary (12 dimensions per residue)\n",
    "aa_properties = {}\n",
    "canonical_set = set(kd_hydro.keys())\n",
    "for aa in canonical_set:\n",
    "    hydro_norm  = (kd_hydro[aa] + 4.5) / 9.0\n",
    "    volume_norm = sidechain_volume[aa] / 227.8\n",
    "    pol_norm    = (polarity[aa] - 4.9) / (13.0 - 4.9)\n",
    "    helix_norm  = choufa_helix[aa] / 1.51\n",
    "    sheet_norm  = choufa_sheet[aa] / 1.70\n",
    "    asa_norm    = (rel_ASA[aa] - 0.54) / (1.48 - 0.54)\n",
    "    aromatic    = 1 if aa in ('F','Y','W') else 0\n",
    "\n",
    "    aa_properties[aa] = [\n",
    "        hydro_norm,          # [0]\n",
    "        charge[aa],          # [1]\n",
    "        h_donors[aa],        # [2]\n",
    "        h_acceptors[aa],     # [3]\n",
    "        flexibility[aa],     # [4]\n",
    "        volume_norm,         # [5]\n",
    "        pol_norm,            # [6]\n",
    "        aromatic,            # [7]\n",
    "        helix_norm,          # [8]\n",
    "        sheet_norm,          # [9]\n",
    "        asa_norm,            # [10]\n",
    "        beta_branched[aa]    # [11]\n",
    "    ]\n",
    "\n",
    "# ─── (B) Load FASTA sequences ─────────────────────────────────────────────────\n",
    "def load_fasta(filepath, filter_non_canonical=False):\n",
    "    seqs = []\n",
    "    with open(filepath) as f:\n",
    "        header = None\n",
    "        seq = \"\"\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None and seq:\n",
    "                    if (not filter_non_canonical) or (set(seq) <= canonical_set):\n",
    "                        seqs.append(seq)\n",
    "                header = line\n",
    "                seq = \"\"\n",
    "            else:\n",
    "                seq += line\n",
    "        if header is not None and seq:\n",
    "            if (not filter_non_canonical) or (set(seq) <= canonical_set):\n",
    "                seqs.append(seq)\n",
    "    return seqs\n",
    "\n",
    "pdb_seqs    = load_fasta(\"pdb_chains.fasta\",   filter_non_canonical=True)   # 70 PDB chains\n",
    "disprot_seqs = load_fasta(\"disprot_13000.fasta\", filter_non_canonical=False)  # 100 DisProt\n",
    "\n",
    "# ─── (C) Compute each chain’s 7 global features ────────────────────────────────\n",
    "def compute_global_features(sequence):\n",
    "    props = []\n",
    "    for aa in sequence:\n",
    "        if aa in aa_properties:\n",
    "            v = aa_properties[aa]\n",
    "            props.append([\n",
    "                v[0],               # hydrophobicity_norm\n",
    "                v[1],               # charge\n",
    "                v[2] + v[3],        # h_dh_a\n",
    "                v[4] / 0.544,       # norm_flex (raw_flex/0.544)\n",
    "                v[6],               # pol_norm\n",
    "                v[7] + v[8],        # arom_plus_helix\n",
    "                v[10]               # asa_norm\n",
    "            ])\n",
    "    if not props:\n",
    "        return np.zeros(7)\n",
    "    return np.mean(np.vstack(props), axis=0)\n",
    "\n",
    "all_features = []\n",
    "all_labels   = []\n",
    "\n",
    "for seq in pdb_seqs:\n",
    "    all_features.append(compute_global_features(seq))\n",
    "    all_labels.append(1)   # 1 = folded (PDB)\n",
    "for seq in disprot_seqs:\n",
    "    all_features.append(compute_global_features(seq))\n",
    "    all_labels.append(0)   # 0 = disordered (DisProt)\n",
    "\n",
    "df_feat = pd.DataFrame(\n",
    "    all_features,\n",
    "    columns=[\n",
    "        \"hydro_norm\",\n",
    "        \"charge\",\n",
    "        \"h_dh_a\",\n",
    "        \"norm_flex\",\n",
    "        \"pol_norm\",\n",
    "        \"arom_plus_helix\",\n",
    "        \"asa_norm\"\n",
    "    ]\n",
    ")\n",
    "df_feat[\"label\"] = all_labels\n",
    "\n",
    "# ─── (D) Compute midpoint thresholds (mean of PDB vs. mean of DisProt) ───────\n",
    "means = df_feat.groupby(\"label\").mean().rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "midpoints = {col: (means.loc[\"PDB\", col] + means.loc[\"DisProt\", col]) / 2\n",
    "             for col in df_feat.columns[:-1]}\n",
    "\n",
    "print(\"Global Feature Means (DisProt vs. PDB):\\n\")\n",
    "print(means, \"\\n\")\n",
    "print(\"Chosen Midpoint Thresholds:\\n\")\n",
    "for feat, t in midpoints.items():\n",
    "    print(f\"  {feat:18s} = {t:.3f}\")\n",
    "print()\n",
    "\n",
    "# ─── (E) Count how many of the 7 conditions each chain satisfies ───────────────\n",
    "def count_conditions(row):\n",
    "    c1 = row[\"hydro_norm\"]          >= midpoints[\"hydro_norm\"]\n",
    "    c2 = abs(row[\"charge\"])         <= abs(midpoints[\"charge\"])\n",
    "    c3 = row[\"h_dh_a\"]              <= midpoints[\"h_dh_a\"]\n",
    "    c4 = row[\"norm_flex\"]           <= midpoints[\"norm_flex\"]\n",
    "    c5 = row[\"pol_norm\"]            <= midpoints[\"pol_norm\"]\n",
    "    c6 = row[\"arom_plus_helix\"]     >= midpoints[\"arom_plus_helix\"]\n",
    "    c7 = row[\"asa_norm\"]            <= midpoints[\"asa_norm\"]\n",
    "    return sum([c1, c2, c3, c4, c5, c6, c7])\n",
    "\n",
    "df_feat[\"conditions_met\"] = df_feat.apply(count_conditions, axis=1)\n",
    "\n",
    "# Show the distribution of “conditions_met” separately for PDB vs. DisProt\n",
    "dist = df_feat.groupby(\"label\")[\"conditions_met\"] \\\n",
    "              .value_counts() \\\n",
    "              .unstack(fill_value=0) \\\n",
    "              .rename(index={0:\"DisProt\", 1:\"PDB\"})\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"Distribution of ‘conditions_met’ by Label:\\n\")\n",
    "print(dist, \"\\n\")\n",
    "\n",
    "# ─── (F) For each k=1…7, classify “folded if conditions_met ≥ k” ─────────────\n",
    "results = []\n",
    "for k in range(1, 8):\n",
    "    preds = (df_feat[\"conditions_met\"] >= k).astype(int)\n",
    "    tp = ((preds == 1) & (df_feat[\"label\"] == 1)).sum()\n",
    "    fn = ((preds == 0) & (df_feat[\"label\"] == 1)).sum()\n",
    "    tn = ((preds == 0) & (df_feat[\"label\"] == 0)).sum()\n",
    "    fp = ((preds == 1) & (df_feat[\"label\"] == 0)).sum()\n",
    "    acc = (tp + tn) / len(df_feat)\n",
    "    results.append({\n",
    "        \"k (min # of features)\": k,\n",
    "        \"TP\": tp,\n",
    "        \"FN\": fn,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"Accuracy\": f\"{acc:.2%}\"\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"Performance as we vary k = minimum # of satisfied conditions:\\n\")\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188e4f0-0482-4ee4-b99d-b1292c9b0761",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "# 3.2 Rule‐Based Seven‐Feature Classifier (Using Previously Learned LR Weights)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ─── (A) Build aa_properties dictionary ───────────────────────────────────────\n",
    "# We reuse exactly the same per‐residue dictionary of 7 features as before.\n",
    "# Each amino acid maps to a 7‐element list:\n",
    "#   [hydro_norm, charge,  h_dh_a,  norm_flex,  pol_norm,  arom_plus_helix,  asa_norm]\n",
    "\n",
    "kd_hydro = {\n",
    "    'A':  1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C':  2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I':  4.5,\n",
    "    'L':  3.8, 'K': -3.9, 'M':  1.9, 'F':  2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V':  4.2\n",
    "}\n",
    "charge = {\n",
    "    'A':  0, 'R':  1, 'N':  0, 'D': -1, 'C':  0,\n",
    "    'Q':  0, 'E': -1, 'G':  0, 'H':  0, 'I':  0,\n",
    "    'L':  0, 'K':  1, 'M':  0, 'F':  0, 'P':  0,\n",
    "    'S':  0, 'T':  0, 'W':  0, 'Y':  0, 'V':  0\n",
    "}\n",
    "h_donors = {'A':0,'R':2,'N':2,'D':0,'C':0,'Q':2,'E':0,'G':0,'H':1,'I':0,\n",
    "            'L':0,'K':1,'M':0,'F':0,'P':0,'S':1,'T':1,'W':1,'Y':1,'V':0}\n",
    "h_acceptors = {'A':0,'R':0,'N':2,'D':2,'C':1,'Q':2,'E':2,'G':0,'H':1,'I':0,\n",
    "               'L':0,'K':0,'M':0,'F':0,'P':0,'S':1,'T':1,'W':0,'Y':1,'V':0}\n",
    "flexibility = {\n",
    "    'A': 0.357, 'R': 0.529, 'N': 0.463, 'D': 0.511, 'C': 0.346,\n",
    "    'Q': 0.493, 'E': 0.497, 'G': 0.544, 'H': 0.323, 'I': 0.462,\n",
    "    'L': 0.365, 'K': 0.466, 'M': 0.295, 'F': 0.314, 'P': 0.509,\n",
    "    'S': 0.507, 'T': 0.444, 'W': 0.305, 'Y': 0.420, 'V': 0.386\n",
    "}\n",
    "sidechain_volume = {\n",
    "    'A':  88.6, 'R': 173.4, 'N': 114.1, 'D': 111.1, 'C': 108.5,\n",
    "    'Q': 143.8, 'E': 138.4, 'G':  60.1, 'H': 153.2, 'I': 166.7,\n",
    "    'L': 166.7, 'K': 168.6, 'M': 162.9, 'F': 189.9, 'P': 112.7,\n",
    "    'S':  89.0, 'T': 116.1, 'W': 227.8, 'Y': 193.6, 'V': 140.0\n",
    "}\n",
    "polarity = {\n",
    "    'A':  8.1, 'R': 10.5, 'N': 11.6, 'D': 13.0, 'C':  5.5,\n",
    "    'Q': 10.5, 'E': 12.3, 'G':  9.0, 'H': 10.4, 'I':  5.2,\n",
    "    'L':  4.9, 'K': 11.3, 'M':  5.7, 'F':  5.2, 'P':  8.0,\n",
    "    'S':  9.2, 'T':  8.6, 'W':  5.4, 'Y':  6.2, 'V':  5.9\n",
    "}\n",
    "choufa_helix = {\n",
    "    'A': 1.45, 'R': 0.79, 'N': 0.73, 'D': 1.01, 'C': 0.77,\n",
    "    'Q': 1.17, 'E': 1.51, 'G': 0.53, 'H': 1.00, 'I': 1.08,\n",
    "    'L': 1.34, 'K': 1.07, 'M': 1.20, 'F': 1.12, 'P': 0.59,\n",
    "    'S': 0.79, 'T': 0.82, 'W': 1.14, 'Y': 0.61, 'V': 1.06\n",
    "}\n",
    "choufa_sheet = {\n",
    "    'A': 0.97, 'R': 0.90, 'N': 0.65, 'D': 0.54, 'C': 1.30,\n",
    "    'Q': 1.23, 'E': 0.37, 'G': 0.75, 'H': 0.87, 'I': 1.60,\n",
    "    'L': 1.22, 'K': 0.74, 'M': 1.67, 'F': 1.28, 'P': 0.62,\n",
    "    'S': 0.72, 'T': 1.20, 'W': 1.19, 'Y': 1.29, 'V': 1.70\n",
    "}\n",
    "rel_ASA = {\n",
    "    'A': 0.74, 'R': 1.48, 'N': 1.14, 'D': 1.23, 'C': 0.86,\n",
    "    'Q': 1.36, 'E': 1.26, 'G': 1.00, 'H': 0.91, 'I': 0.59,\n",
    "    'L': 0.61, 'K': 1.29, 'M': 0.64, 'F': 0.65, 'P': 0.71,\n",
    "    'S': 1.42, 'T': 1.20, 'W': 0.55, 'Y': 0.63, 'V': 0.54\n",
    "}\n",
    "beta_branched = {aa: (1 if aa in ('V','I','T') else 0) for aa in kd_hydro.keys()}\n",
    "\n",
    "aa_properties = {}\n",
    "canonical_set = set(kd_hydro.keys())\n",
    "for aa in canonical_set:\n",
    "    # Normalize hydrophobicity to [0,1]\n",
    "    hydro_norm  = (kd_hydro[aa] + 4.5) / 9.0\n",
    "    # Normalize sidechain volume (not used directly in the 7‐feature vector)\n",
    "    volume_norm = sidechain_volume[aa] / 227.8\n",
    "    # Normalize polarity → [0,1]\n",
    "    pol_norm    = (polarity[aa] - 4.9) / (13.0 - 4.9)\n",
    "    # Normalize helix propensity → [0,1]\n",
    "    helix_norm  = choufa_helix[aa] / 1.51\n",
    "    # Normalize sheet propensity → [0,1]\n",
    "    sheet_norm  = choufa_sheet[aa] / 1.70\n",
    "    # Normalize ASA → [0,1]\n",
    "    asa_norm    = (rel_ASA[aa] - 0.54) / (1.48 - 0.54)\n",
    "    # Aromatic indicator (F, Y, W)\n",
    "    aromatic    = 1 if aa in ('F','Y','W') else 0\n",
    "\n",
    "    # Our final 7 features per residue:\n",
    "    aa_properties[aa] = [\n",
    "        hydro_norm,                    # [0] normalized hydrophobicity\n",
    "        charge[aa],                    # [1] net charge\n",
    "        h_donors[aa] + h_acceptors[aa],# [2] total H-bond donors+acceptors\n",
    "        flexibility[aa] / 0.544,       # [3] normalized flexibility (max=0.544)\n",
    "        pol_norm,                      # [4] normalized polarity\n",
    "        aromatic + helix_norm,         # [5] aromatic + helix propensity\n",
    "        asa_norm                       # [6] normalized solvent-accessible surface\n",
    "    ]\n",
    "\n",
    "# ─── (B) Load FASTA sequences ─────────────────────────────────────────────────\n",
    "def load_fasta(filepath, filter_non_canonical=False):\n",
    "    \"\"\"\n",
    "    Read all sequences from a FASTA file.\n",
    "    If filter_non_canonical=True, discard any sequence containing non-standard AAs.\n",
    "    \"\"\"\n",
    "    seqs = []\n",
    "    with open(filepath) as f:\n",
    "        header = None\n",
    "        seq = \"\"\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None and seq:\n",
    "                    # Only keep this sequence if all residues are in our 20‐AA set,\n",
    "                    # when filter_non_canonical=True.\n",
    "                    if (not filter_non_canonical) or (set(seq) <= canonical_set):\n",
    "                        seqs.append(seq)\n",
    "                header = line\n",
    "                seq = \"\"\n",
    "            else:\n",
    "                seq += line\n",
    "        # Catch the last sequence\n",
    "        if header is not None and seq:\n",
    "            if (not filter_non_canonical) or (set(seq) <= canonical_set):\n",
    "                seqs.append(seq)\n",
    "    return seqs\n",
    "\n",
    "# Load ~70 folded PDB chains (filter non-canonical AA)\n",
    "pdb_seqs     = load_fasta(\"pdb_chains.fasta\",    filter_non_canonical=True)\n",
    "# Load ~13k DisProt sequences (allow non-canonical AA)\n",
    "disprot_seqs = load_fasta(\"disprot_13000.fasta\", filter_non_canonical=False)\n",
    "\n",
    "# ─── (C) Compute each sequence’s 7 global features ─────────────────────────────\n",
    "def compute_global_features(sequence):\n",
    "    \"\"\"\n",
    "    For a given AA sequence, compute a 7‐element array:\n",
    "      [mean_hydro_norm, mean_charge, mean_h_dh_a,\n",
    "       mean_norm_flex,  mean_pol_norm,  mean_arom_plus_helix,  mean_asa_norm]\n",
    "    by averaging per-residue aa_properties.\n",
    "    \"\"\"\n",
    "    props = []\n",
    "    for aa in sequence:\n",
    "        if aa in aa_properties:\n",
    "            props.append(aa_properties[aa])\n",
    "    if not props:\n",
    "        # If the sequence is empty or has no canonical AA, return zeros\n",
    "        return np.zeros(7)\n",
    "    return np.mean(np.vstack(props), axis=0)\n",
    "\n",
    "# Build feature matrix (one row per protein) and label vector\n",
    "all_features = []\n",
    "all_labels   = []\n",
    "\n",
    "for seq in pdb_seqs:\n",
    "    all_features.append(compute_global_features(seq))\n",
    "    all_labels.append(1)  # 1 = folded (PDB)\n",
    "for seq in disprot_seqs:\n",
    "    all_features.append(compute_global_features(seq))\n",
    "    all_labels.append(0)  # 0 = disordered (DisProt)\n",
    "\n",
    "df_feat = pd.DataFrame(\n",
    "    all_features,\n",
    "    columns=[\n",
    "        \"hydro_norm\",        # normalized hydrophobicity\n",
    "        \"charge\",            # net charge\n",
    "        \"h_dh_a\",            # total H-bond donors + acceptors\n",
    "        \"norm_flex\",         # normalized flexibility\n",
    "        \"pol_norm\",          # normalized polarity\n",
    "        \"arom_plus_helix\",   # aromatic + helix propensity\n",
    "        \"asa_norm\"           # normalized ASA\n",
    "    ]\n",
    ")\n",
    "df_feat[\"label\"] = all_labels\n",
    "\n",
    "# ─── (D) Logistic Regression “Rule” Using Learned Weights ────────────────────\n",
    "# We previously trained a logistic model and obtained these weights:\n",
    "#   hydro_norm      → +9.149\n",
    "#   charge          → +3.051\n",
    "#   h_dh_a          → +2.034\n",
    "#   norm_flex       → –7.553\n",
    "#   pol_norm        → –6.521\n",
    "#   arom_plus_helix → +8.728\n",
    "#   asa_norm        → –7.629\n",
    "# Intercept = +0.131\n",
    "#\n",
    "# The sigmoid(score) = 1 / (1 + exp( – (intercept + Σ weight_i × feature_i) )).\n",
    "# We predict “folded” (PDB) if sigmoid(score) > 0.5.\n",
    "\n",
    "# Store weights and intercept in numpy arrays for easy dot‐product\n",
    "weights = np.array([\n",
    "    9.149,    # weight for hydro_norm\n",
    "    3.051,    # weight for charge\n",
    "    2.034,    # weight for h_dh_a\n",
    "   -7.553,    # weight for norm_flex\n",
    "   -6.521,    # weight for pol_norm\n",
    "    8.728,    # weight for arom_plus_helix\n",
    "   -7.629     # weight for asa_norm\n",
    "])\n",
    "intercept = 0.131\n",
    "\n",
    "# Compute “score” and predicted probability for each protein in df_feat\n",
    "# sigmoid(x) = 1/(1 + exp(–x))\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# Extract feature matrix (N × 7)\n",
    "X = df_feat[[\n",
    "    \"hydro_norm\",\n",
    "    \"charge\",\n",
    "    \"h_dh_a\",\n",
    "    \"norm_flex\",\n",
    "    \"pol_norm\",\n",
    "    \"arom_plus_helix\",\n",
    "    \"asa_norm\"\n",
    "]].values\n",
    "\n",
    "# Compute raw scores: intercept + X ⋅ weights\n",
    "raw_scores = intercept + X.dot(weights)\n",
    "\n",
    "# Compute predicted probabilities of “PDB” (folded)\n",
    "probs_pdb = sigmoid(raw_scores)\n",
    "\n",
    "# Choose threshold = 0.5 for “PDB” vs. “DisProt”\n",
    "preds_05 = (probs_pdb > 0.5).astype(int)\n",
    "\n",
    "# ─── (E) Evaluate on the Entire Dataset ───────────────────────────────────────\n",
    "true_labels = df_feat[\"label\"].values\n",
    "\n",
    "print(\"Classification Report (Threshold = 0.5):\\n\")\n",
    "print(classification_report(true_labels, preds_05, target_names=[\"DisProt\",\"PDB\"]))\n",
    "\n",
    "cm = confusion_matrix(true_labels, preds_05)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual DisProt\", \"Actual PDB\"],\n",
    "    columns=[\"Pred DisProt\", \"Pred PDB\"]\n",
    ")\n",
    "print(\"Confusion Matrix (Threshold = 0.5):\\n\")\n",
    "print(cm_df)\n",
    "\n",
    "# ─── (F) Optionally, Adjust Threshold to Reduce False Positives ──────────────\n",
    "# Because PDB is very rare, you may want to require a higher probability (e.g., 0.7) \n",
    "# to call “PDB.” Simply do: preds_07 = (probs_pdb > 0.7).astype(int) and re‐evaluate.\n",
    "\n",
    "# Example at threshold = 0.7:\n",
    "preds_07 = (probs_pdb > 0.7).astype(int)\n",
    "print(\"\\n--- Threshold = 0.7 ---\")\n",
    "print(classification_report(true_labels, preds_07, target_names=[\"DisProt\",\"PDB\"]))\n",
    "cm_07 = confusion_matrix(true_labels, preds_07)\n",
    "cm_07_df = pd.DataFrame(\n",
    "    cm_07,\n",
    "    index=[\"Actual DisProt\", \"Actual PDB\"],\n",
    "    columns=[\"Pred DisProt\", \"Pred PDB\"]\n",
    ")\n",
    "print(\"Confusion Matrix (Threshold = 0.7):\\n\")\n",
    "print(cm_07_df)\n",
    "\n",
    "# ─── (G) (Optional) Inspect the Learned Weights ──────────────────────────────\n",
    "print(\"\\nLearned Logistic Weights:\")\n",
    "feature_names = [\n",
    "    \"hydro_norm\", \"charge\", \"h_dh_a\",\n",
    "    \"norm_flex\", \"pol_norm\", \"arom_plus_helix\", \"asa_norm\"\n",
    "]\n",
    "for name, w in zip(feature_names, weights):\n",
    "    print(f\"{name:15s} → {w:.3f}\")\n",
    "print(f\"Intercept: {intercept:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
